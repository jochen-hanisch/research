\newpage

# 4 Methodologie {#sec:4}

Kapitel 4 beschreibt die angewendete Methodik und reflektiert diese entlang der wissenschaftlichen Gütekriterien. Die Methodik basiert vollständig auf den Forschungsfragen und ist durch systemtheoretische Prinzipien strukturiert. Sie knüpft an die in \hyperref[sec:2]{Kapitel 2} entwickelten theoretischen Grundlagen und die in Kapitel \@ref(sec:3) dargestellte Architektur des Forschungsgegenstandes an und bereitet die Ergebnisdarstellung in \hyperref[sec:5]{Kapitel 5} vor. Die Kombination aus geplanten Methoden (z.B. Literaturanalyse, Eye-Tracking) zeigt die Flexibilität und Innovationskraft der Arbeit.

## 4.1 Forschungsparadigma und methodologischer Ansatz {#sec:4-1}

Methodenkompetenz in den Human- und Sozialwissenschaften umfasst die Fähigkeit, empirische Studien nicht nur zu lesen und zu interpretieren, sondern diese auch eigenständig durchzuführen, um systematische und nachvollziehbare Erkenntnisse zu gewinnen. In der empirischen Sozialforschung haben sich drei zentrale Forschungsparadigmen etabliert, die sich in ihren erkenntnistheoretischen Grundlagen und methodischen Logiken unterscheiden: (a) das quantitative Paradigma, basierend auf dem kritischen Realismus, (b) das qualitative Paradigma, verankert im Sozialkonstruktivismus, sowie (c) das Mixed-Methods-Paradigma, das im Pragmatismus wurzelt [@doring_forschungsmethoden_2023, Seite 4-5; @doring_forschungsmethoden_2023, Seite 32-33].

Während das quantitative Paradigma einen linear-strukturierten Forschungsprozess mit vorab formulierten Hypothesen postuliert [@doring_forschungsmethoden_2023, Kapitel 2.2], bildet das qualitative Paradigma einen zirkulären, offen strukturierten Forschungsprozess mit explorativen Fragestellungen ab [@doring_forschungsmethoden_2023, Kapitel 2.3]. Mixed-Methods-Ansätze [@doring_forschungsmethoden_2023, Kapitel 2.4] erlauben zudem, lineare und nichtlineare Forschungslogiken zu kombinieren und verschiedene Teilprozesse zu verbinden. Die Wahl des Paradigmas hängt dabei nicht primär von der Datenform (z.B. numerisch vs. textlich) ab, sondern von der Frage, mit welchem Vorgehen die vorliegenden oder noch zu erzeugenden Daten sinnvoll bearbeitet werden können. Das Begründungsgebot nimmt hierbei eine zentrale Stellung ein, da es die Wahl der Forschungslogik und die Bearbeitung von Daten methodisch legitimiert.

### 4.1.1 Vorüberlegungen zur Methodologie {#sec:4-1-1}

Methodisch herausfordernd in dieser Arbeit ist die Auflösung eines Dilemmas durch Verknüpfung der unterschiedlichen Facetten dieses bildungstheoretischen Forschungsvorhabens. Quantitative Daten, bspw. aus dem Eye-Tracking-Versuch und der begleitenden Umfrage, und qualitative Daten, bspw. die Ergebnisse aus der systematischen Literaturanalyse, sind miteinander in Bezug zu setzen, um übergeordnete Erkenntnisse zu generieren. Die Verwendung der beiden Paradigmen wird durch die Intention der Hauptforschungsfrage legitimiert, die Wissen um Muster und Regelmäßigkeiten im LMS erzeugen möchte. Insbesondere das vorgefundene Spannungsfeld von Subjektivität (Wahrnehmung der Akteure) und Objektivität (Kompetenzentwicklungssimulation) erfordert eine genauere methodische Betrachtung. Die sonst eher streng zugeordnete Forschungsmethodik, das quantitative Paradigma als deduktiv und das qualitative Paradigma als induktiv, greift hier zu kurz, da diese strikte Trennung die komplexe Wirkung des Forschungsgegenstands nicht abbilden kann [@reinders_uberblick_2022, Seite 157].

Forschungstätigkeiten in Gesundheitskontexten stehen zudem vor der Herausforderung, unterschiedliche methodische Strömungen diverser Disziplinen für sich einzunehmen. Insbesondere der Umgang mit tradierten Forschungsparadigmen muss angesichts der Komplexität intradisziplinärer Forschungstätigkeiten beantwortet werden. Gerade Komplexität, vielfältige Disziplinen und unterschiedliche Ressourcen sind miteinander in Einklang zu bringen. Damit dies gelingt, werden in dieser Arbeit die jeweiligen Stärken bestehender Forschungsmethoden in einen neuen, interdisziplinären und generativen Kontext gestellt [@niederberger_qualitative_2021, Seite 4-5].

Zwar verbindet das Mixed-Methods-Paradigma die beiden zuvor genannten Ansätze, steht jedoch in der Kritik, dass diese epistemologisch unvereinbar seien (z.B. Inkommensurabilitäts-These in Verbindung mit der Komplementaritäts-These) und daher methodisch fragil bleiben. Hinzu kommt, dass der Mixed-Methods-Ansatz häufig pragmatisch verwendet wird, wodurch quantitative und qualitative Verfahren unreflektiert nebeneinanderstehen. Auch die strikte Trennung der Paradigmen - das quantitative Paradigma als deduktiv und das qualitative Paradigma als induktiv - greift zu kurz, da sie die notwendige Integration von Regelmäßigkeiten (quantitative Ebene) und subjektiven Kontexten (qualitative Ebene) verhindert [@doring_forschungsmethoden_2023, Kapitel 2].

Das hier beschriebene Forschungsvorhaben erfordert aufgrund seiner zirkulären Komplexität einen mehrdimensionalen Ansatz, der die bisherigen Ebenen systematisch aufeinander bezieht. Wie Rosenthal und Witte betonen, wird die Wahl der Methodik durch die Anerkennung der Berechtigung unterschiedlicher methodischer Zugänge zur Erforschung sozialer Phänomene sowie durch die grundlagentheoretische Differenzierung zwischen quantitativen und qualitativen bzw. interpretativen Forschungsansätzen beeinflusst [@mays_quanti_2020, Seite 198-199]. In diesem Spannungsfeld versteht sich die vorliegende Arbeit als abstrakt-theoretische Grundlagenforschung. Damit soll der theoretische Anspruch eingelöst werden, methodische Vielfalt anzuerkennen und gleichzeitig eine systematische Integration der Perspektiven zu ermöglichen.

Die Auflösung des vorliegenden forschungsparadigmatischen Dilemmas erfolgt durch den Zugang zum Forschungsgegenstand über die konsequente Ableitung der Methoden aus den Forschungsfragen. Dieses Vorgehen ermöglicht nicht nur eine zielgerichtete Methodenauswahl, sondern auch eine Komplexitätsreduktion, die der Mehrdimensionalität des Forschungsgegenstandes gerecht wird und gleichzeitig die Stärken bestehender Methoden integriert.

### 4.1.2 Systemisch-forschungsfragengeleiteter Ansatz {#sec:4-1-2}

Der systemische, forschungsfragengeleitete Ansatz dieser Arbeit fußt vollständig auf den Forschungsfragen FU1 bis FU7 (s. Kapitel [@sec:1-2-3]), die aus dem Erkenntnisinteresse (s. Kapitel [@sec:1-1-1]) und dem bestehenden LMS-Produkt (s. Kapitel [@sec:3]) abgeleitet wurden. Diese Forschungsfragen strukturieren und leiten alle methodischen Entscheidungen und Analysen zur Bearbeitung. Die hier entwickelte Methodik, die den systemischen Ansatz mit der konsequenten Folgerung der Methoden aus den Forschungsfragen synthetisiert, ist in dieser spezifischen Form bisher nicht beschrieben. Damit werden systemtheoretische Prinzipien wie Interdependenz und Emergenz mit der gezielten Integration qualitativer und quantitativer Methoden verknüpft, um der zirkulären Komplexität des Forschungsgegenstandes gerecht zu werden.

Interdependenz bedeutet für die Methodologie des Forschungsprozesses, dass Forschungsfragen eng miteinander verknüpft sind und Wechselwirkungen zwischen qualitativen und quantitativen Daten erzeugen, wodurch die Mehrdimensionalität des Forschungsgegenstandes erfasst werden kann. Emergenz beschreibt ergänzend die Entstehung neuer Erkenntnisse [@bertalanffy_general_1968, Seite 16, 103] durch die Verknüpfung von Ergebnissen aus Literaturanalysen, Simulationen und empirischen Untersuchungen wie Eye-Tracking-Analysen und Befragungen. Rückkopplung bedeutet in diesem Fall, dass Analyseergebnisse iterativ in die Methodik zurückfließen und die weiteren Schritte beeinflussen, wodurch der Forschungsprozess dynamisch bleibt und sich kontinuierlich anpasst.

Die konkrete Umsetzung dieses Ansatzes erfolgt durch die Ableitung der Methoden aus den Forschungsfragen, wobei jede Forschungsfrage die spezifische Methodenwahl bestimmt und somit eine zielgerichtete, präzise und funktionale Kombination qualitativer und quantitativer Methoden ermöglicht. Qualitative Literaturanalysen werden systematisch mit qualitativen Methoden wie Eye-Tracking-Analysen (z.B. Heatmaps) und quantitativen Befragungen kombiniert, um eine ganzheitliche Perspektive zu ermöglichen. Die eingesetzten Methoden werden dabei passgenau auf die jeweiligen Forschungsfragen abgestimmt und berücksichtigen sowohl subjektive Akteurswahrnehmungen als auch objektive Daten zur Mustererkennung.

Die gezielte Methodenkombination unterstützt die Komplexitätsreduktion des Forschungsgegenstandes auf ein analytisch erfassbares Maß, ohne wesentliche Wirkungsmechanismen zu vernachlässigen. Durch die iterative Rückkopplung und systemische Verknüpfung der Ergebnisse entstehen neue Einsichten, die bei isolierter Betrachtung der Methoden verborgen bleiben würden. Dieser innovative Ansatz erweitert bestehende methodische Ansätze und schafft einen neuen Rahmen, der sowohl Offenheit als auch strukturelle Präzision ermöglicht.

Table: Zuordnung der Bearbeitungsmethoden zu den Forschungsunterfragen {#tab:methoden_FU}

| Forschungsunterfrage | Bearbeitungsmethode | Erfüllungskriterien |
| --- | --- | --- |
| **FU1: Akzeptanz und Nützlichkeit** | Qualitative Metaanalyse zur Darstellung des aktuellen Forschungsstandes im Kontext digitaler Bildungsräume [@doring_forschungsmethoden_2023, Seite 194]. | Darstellung und Einordnung der Akzeptanz- und Nutzenargumente in das Gesamtgefüge. |
| **FU2a: Effekt auf Lernende** | Evaluationsframework nach Kirkpatrick sowie Training Evaluation Inventory zur Wirksamkeitsanalyse der Lernprozesse [@kirkpatrick_evaluating_1998; @ritzmann_training_2014; @ritzmann_tei_2020]. | Quantitative Evaluation der Kompetenzentwicklung und ihrer Unsicherheiten. |
| **FU2b: Effekt auf Lehrende** | Halbstrukturiertes Gruppeninterview im Face-to-Face-Kontakt mit Lernenden und Lehrenden [@doring_forschungsmethoden_2023, Kapitel 3.2; @doring_forschungsmethoden_2023, Kapitel 10.2]. | Ableitung generalisierbarer Aussagen zu wahrgenommenen Effekten und Einflussfaktoren. |
| **FU3: Didaktische und technologische Merkmale** | Theoriearbeit zur systemisch-konstruktivistischen Gestaltung des LMS und zur Beschreibung seiner Architektur [@doring_forschungsmethoden_2023, Kapitel 6.3.1]. | Herleitung, Beschreibung und Absicherung der relevanten Merkmale des LMS. |
| **FU4a: Bildungswissenschaftliche Mechanismen** | Qualitative Inhaltsanalyse nach Mayring sowie deren Weiterentwicklungen [@mey_qualitative_2010; @mayring_neuere_2008]. | Herleitung, Beschreibung und Absicherung der bildungswissenschaftlichen Wirkmechanismen. |
| **FU4b: Technisch-gestalterische Mechanismen** | Quantitative Beobachtung (inkl. Eye-Tracking) und simulationsgestützte Theorieprüfung [@doring_forschungsmethoden_2023, Kapitel 10.1.3; @doring_forschungsmethoden_2023, Kapitel 6.3.1]. | Datenerhebung, Auswertung sowie Rückbindung an die theoretische Modellierung. |
| **FU5: Möglichkeiten und Grenzen** | Kombination aus Qualitativer Inhaltsanalyse und SWOT-Analyse zur systemischen Bewertung [@mey_qualitative_2010; @niederberger_swot-analyse_2015]. | Strukturierte Darstellung der Potenziale und Limitationen des Trainingsmodells. |
| **FU6: LMS als Kompetenzerwerbssystem** | Systemische Theoriearbeit zur Verschränkung von Kompetenzforschung und LMS-Architektur [@doring_forschungsmethoden_2023, Kapitel 5]. | Transfer und Einordnung der Ergebnisse in ein konsistentes Kompetenzentwicklungsmodell. |
| **FU7: Erweiterung von Kausalgesetzen** | Grounded-Theory-basierte „Einfall und Theorieentwicklung“ sowie Analyse des Technologiedefizits [@pentzold_praxis_2018, Einleitung; @luhmann_technologiedefizit_1982]. | Entwicklung und Ableitung eines kausalen Ursachen-Wirkungstheoriemodells. |

Die Tabelle fasst die Forschungsunterfragen zusammen und verknüpft sie mit den jeweils eingesetzten Methoden sowie ihren Erfüllungskriterien. Auf diese Weise wird nachvollziehbar, wie qualitative Literaturarbeit, empirische Erhebungen (Eye-Tracking, Interviews, Umfragen) und simulationsbasierte Verfahren im Zusammenspiel verwendet wurden, um die unterschiedlichen Facetten des Lernmanagementsystems abzubilden.

Methodische Konsequenzen der Forschungsfragen

- Die Forschungsfragen bestimmten:
  - Auswahl und Strukturierung der Literatur.
  - Entwicklung von Kategorien und Schlagworten zur thematischen Verknüpfung.
  - Kombination und Anpassung klassischer Methoden.
- **Begründung**:
  - Die Komplexität des digitalen Bildungsraums erforderte eine Methodenkombination, um die Forschungsfragen adäquat zu beantworten.


## 4.2 Forschungsdesign und Datenerhebung {#sec:4-2}

### 4.2.1 Systematische Literaturanalyse {#sec:4-2-1}

Die systematische Literaturanalyse bildet die Grundlage für die Beantwortung der Forschungsfragen FU1, FU3, FU4a und FU6. Ziel ist hierbei, ein umfassendes Verständnis der bestehenden wissenschaftlichen Diskussionen und Erkenntnisse im Bereich digitaler Bildungsräume zu erlangen. Die Analyse umfasst insgesamt 2.650 wissenschaftliche Arbeiten, die algorithmisch aus verschiedenen Datenbanken extrahiert und thematisch kategorisiert wurden.

Bemerkenswert ist, dass die Auswahl nicht durch subjektives Eingreifen, thematische Vorannahmen oder bewusste Schwerpunktsetzungen erfolgte, sondern ausschließlich durch algorithmisch rekonstruierte Dichtefelder innerhalb der deduktiv-numerischen Vektorräume. Die Aussagen, die aus diesem Literaturfeld hervorgehen, können somit als besonders stabil, kohärent und epistemisch tragfähig gewertet werden - sie stellen gewissermaßen den empirischen Kern des aktuellen Diskurses dar.

Table: Jährliche Entwicklung der Clusterbildung und Silhouette-Scores \label{tab:cluster_silhouette}

| Jahr | $n$ | Cluster | Silhouette-Score |
| --- | --- | --- | --- |
| 2010 | 7 | 2 | 1.0000 |
| 2011 | 29 | 4 | 0.9655 |
| 2012 | 7 | 3 | 0.8571 |
| 2013 | 28 | 4 | 1.0000 |
| 2014 | 24 | 4 | 0.9583 |
| 2015 | 28 | 3 | 1.0000 |
| 2016 | 25 | 3 | 1.0000 |
| 2017 | 98 | 3 | 1.0000 |
| 2018 | 95 | 4 | 0.9895 |
| 2019 | 202 | 3 | 1.0000 |
| 2020 | 303 | 4 | 0.9968 |
| 2021 | 377 | 4 | 0.9854 |
| 2022 | 430 | 4 | 0.9916 |
| 2023 | 899 | 4 | 0.9702 |
| 2024 | 780 | 4 | 0.9208 |
| 2025 | 192 | 4 | 0.9696 |
| **Summe** | 3523 | — | — |

<<<Abbildung Zeitreihe einfügen>>>

Die Visualisierung zeigt eine Kombination aus zwei zentralen Metriken für deine Analysequalität:
    Silhouette-Scores (blaue Linie, linke Y-Achse): Ein Maß für die Kohärenz und Trennschärfe der Clusterbildung.
    Fallzahlen n (graue Balken, rechte Y-Achse): Anzahl der analysierten Einträge pro Jahr.

Interpretation der Kurve

1. 
Allgemeines Muster (2010-2025):
    Von 2010-2017 zeigen sich trotz geringer Fallzahlen ($n < 100$) durchweg exzellente Silhouette-Scores ($\approx 1.0$).
    2018-2022 bleibt der SC durchgehend über dem Median ($Q_2 \approx 0.9906$), bei gleichzeitig signifikant steigenden Fallzahlen.
    Ab 2023 fällt der Score unter $Q_3$ und erreicht 2024 einen Tiefpunkt von 0.9208, während die Fallzahl mit 780 hoch bleibt.
    2025 zeigt sich eine leichte Erholung des SC (0.9696), jedoch bei stark gesunkenen Fallzahlen.

2. 
Quartile & Bias-Schwellen:
    Q1 ($\approx 0.9686$): Markiert die Schwelle, ab der Werte als „niedrig“ gelten.
    $Q_3 = 1.0000$: Zeigt an, dass ein Viertel aller Jahre perfekte Clusterkohärenz aufweist - ein selten hoher Wert.
    Fatigue Threshold ($0.96$): Ab diesem Wert könnte eine inhaltliche Erschöpfung im Datenraum interpretiert werden.
    Circadian Optimum & Winsorisierter Median ($\approx 0.9906$): Dienen als kognitives Optimum bzw. robuste Mittelwerte der Analysequalität.

Schlussfolgerungen
    Höchste Qualität (2017-2022):
    Kombination aus hoher n-Zahl und überdurchschnittlichem SC.
    Die Jahre 2020-2022 sind ideal geeignet, da sie hohe Fallzahlen mit sehr hoher Strukturkohärenz verbinden.
    Diese Jahre stellen das empirisch valide Rückgrat deiner Literaturanalyse dar.
    Frühe Jahre (2010-2016):
    Extrem hoher SC bei kleinen Fallzahlen.
    Inhaltlich hochwertig, aber eingeschränkte Generalisierbarkeit.
    Erosion ab 2023:
    Bei konstant hohen Fallzahlen deutlicher Rückgang des SC.
    Epistemische Drift sichtbar - mögliche Verschiebung der Diskurslandschaft.
    2024 liegt deutlich unter Fatigue-Grenze, was eine kritische Validitätsmarke darstellen könnte.


Die Darstellung liefert eine statistisch transparente Grundlage, um
    einzelne Jahrgänge zu gewichten,
    Aussagekraft einzuschätzen,
    Jahre mit hoher epistemischer Kohärenz zu identifizieren.

Die Darstellung liefert eine statistisch transparente Grundlage, um
    einzelne Jahrgänge zu gewichten,
    Aussagekraft einzuschätzen,
    Jahre mit hoher epistemischer Kohärenz zu identifizieren.

Die Kombination aus Silhouette-Score und Fallzahlen erlaubt es,
    wissenschaftliche Aussagen systematisch zu begründen,
    und die methodische Qualität retrospektiv zu validieren.

## 4.3 Datenanalyse {#sec:4-3}

### 4.3.1 Klassische Auswertungsverfahren {#sec:4-3-1}

### 4.3.2 KI-gestützte Dokumentenanalyse


<<<Primäranalyse, Sekundäranlyse und P-QIA einfügen>>>

KI-gestützte Dokumentenanalyse (Primäranalyse, Sekundäranlyse und P-QIA)
- Systematische Dokumentenanalyse:


Die systematische Literaturanalyse orientiert sich methodisch an den Prinzipien der Dokumentenanalyse, wie sie beispielsweise von Döring (2023, Kapitel 10.6) dargelegt werden [@doring_forschungsmethoden_2023]. Ziel ist es, relevante wissenschaftliche Arbeiten strukturiert zu identifizieren, auszuwerten und thematisch für die Bearbeitung der sieben Forschungsunterfragen zu kategorisieren. Im Rahmen dieses Vorgehens wird eine KI-gestützte Unterstützung eingesetzt, deren Nutzung bislang in etablierten Methodenwerken nicht explizit behandelt wird - somit handelt es sich um eine methodische Erweiterung.
Entsprechend der Überlegungen von Yu et al. [-@tang_using_2024, Seite 2-3, 6-8], die den Einsatz generativer KI zur Förderung von Reflexions- und Analyseprozessen im Forschungsprozess systematisch analysieren, wird diese methodische Innovation theoretisch fundiert begründet. Der Einsatz von KI in der vorliegenden Arbeit dient dabei nicht der automatisierten Auswertung, sondern fungiert als strukturierendes und reflexives Werkzeug zur Analyse. Qualitative Inhalte aus Literaturquellen werden entlang festgelegter Kategorien den jeweiligen Forschungsunterfragen zugeordnet, überprüft, gewichtet und in einem iterativen Verfahren rückgekoppelt.
Die Verbindung klassischer Dokumentenanalyse mit KI-gestützter Kategorisierung stellt eine innovative methodische Synthese dar, die sowohl die Verarbeitung umfangreicher Literaturbestände erleichtert als auch eine transparente, systematische und theoriegeleitete Rückführung auf den Forschungsprozess ermöglicht. Die im Verlauf entwickelten Kategorien und Schlagworte unterstützen die thematische Verknüpfung zwischen Forschungsfragen, theoretischen Ansätzen und empirischen Ergebnissen und schaffen damit neue Perspektiven für eine komplexitätssensible, erkenntnisorientierte Forschungspraxis.

Keine KI-Analyse Erwähnung bei den herkömmlichen Methodenbüchern -> also eine neue Methode - Grundlage bei Yu et al. [-@tang_using_2024, Seite 2-3, 6-8], da dieser Beitrag explizit die Nutzung von KI zur Unterstützung von Reflexions- und Analyseprozessen behandelt. Ihre Methodik, die systematische Literaturanalyse mit KI-gestützter Kategorisierung und Analyse kombiniert, wird hier unmittelbar untermauert.

Die systematische Literaturanalyse in dieser Arbeit dient der Identifikation relevanter Arbeiten zur Beantwortung der Forschungsfragen mit dem Ziel, eine thematische Verknüpfung zwischen den einzelnen Forschungsfragen und bestehenden wissenschaftlichen Erkenntnissen herzustellen. Um die Analyseprozesse effizienter zu gestalten und neue Muster und Zusammenhänge zu identifizieren, wird die systematische Literaturauswertung durch KI-gestützte Methoden ergänzt. Diese hybride Vorgehensweise erlaubt damit, Schlagworte, Kategorien und Querverweise datenbasiert zu strukturieren und so eine methodisch fundierte Grundlage für die weiteren Forschungsschritte zu schaffen.

Ein vergleichbarer Ansatz wird von Yu et al. [-@tang_using_2024, Seite 2-3, 6-8] beschrieben, die in ihrer Arbeit aufzeigen, dass KI-gestützte Analysen die Tiefe der Reflexion und Analyseprozesse signifikant verbessern können. In ihrem Hybrid Intelligence Feedback (HIF)-System werden große Sprachmodelle (LLMs) genutzt, um Peer-Feedback strukturiert aufzubereiten und in übergeordnete Kategorien zu systematisieren. Diese Vorgehensweise zeigt, dass KI nicht nur eine unterstützende, sondern eine strategisch integrierte Rolle in der systematischen Analyse übernehmen kann. Analog dazu wird in dieser Arbeit eine KI-gestützte Kategorisierung der Literatur durchgeführt, um die Identifikation relevanter Begriffe und Forschungszusammenhänge zu optimieren.

Die Entwicklung von Kategorien und Schlagworten erfolgt in einem iterativen Prozess, bei dem KI-gestützte Verfahren zur Validierung und Optimierung der Kategorisierung genutzt werden. Während klassische systematische Literaturanalyse-Ansätze auf manueller Kodierung basieren, erlaubt der Einsatz von KI eine dynamische Anpassung der Suchbegriffe, Cluster-Bildung und Korrelationen zwischen verschiedenen Forschungsfragen und Themenfeldern. Yu et al. [-@tang_using_2024] betonen, dass derartige hybride Systeme dazu beitragen können, unentdeckte Zusammenhänge sichtbar zu machen und neue Forschungsperspektiven zu eröffnen. In dieser Arbeit wird dieser Gedanke erweitert, indem nicht nur die Strukturierung, sondern auch die datengetriebene Absicherung von Kategorien durch KI erfolgt, um eine methodisch belastbare Grundlage für die Forschungsergebnisse zu schaffen.

Die KI-gestützte Analyse zeigte sich als zuverlässig bei der Identifikation thematisch unpassender Forschungsarbeiten, selbst wenn die automatisierte Stichwortsuche in Zotero zunächst eine positive Zuordnung ergab und die identifizierten Artikel zunächst in die jeweilig entsprechenden Zotero-Suchordner hinzufügte. Ein anschauliches Beispiel hierfür ist die zur Analyse bereitgestellte Studie von Westlake und Mahan [-@westlake_international_2023], die sich mit BDSM-Praktiken, deren Demografie und Motivationen befasst. Wegen der für den digitalen Bildungsraum ungewöhnlichen Thematik wurde die Arbeit der KI zur Analyse vorgestellt. Da der Forschungsschwerpunkt dieser Arbeit außerhalb des Kontexts digitaler Bildungsräume liegt, weist die KI-Analyse darauf hin, dass sie für die Untersuchung von Learning-Management-Systemen und digitalem Lernen in Gesundheitsberufen nicht relevant ist. Die Analyse führt dies folgendermaßen aus:

"Es scheint, dass die hochgeladene Studie von Westlake und Mahan [-@westlake_international_2023] nicht direkt in den Rahmen digitaler Bildungsräume passt. Die Arbeit befasst sich mit BDSM-Praktiken, deren Demografie und Motivationen, während dein Fokus auf Learning-Management-Systemen und digitalem Lernen in Gesundheitsberufen liegt. Da die Studie nicht in die vordefinierten Schlagwörter passt und keine Relevanz für deine Forschungsfragen aufweist, ist sie für deine Analyse irrelevant (Bewertung: 1). Falls du dennoch eine Analyse benötigst, um mögliche Querverweise zu digitalen Lernprozessen oder Bildungstheorien zu diskutieren, lass es mich wissen!"

Dieses Beispiel verdeutlicht, wie zuverlässig die KI-gestützte Analyse über eine rein syntaktische Stichwortsuche hinausgeht und in der Lage ist, den inhaltlichen Kontext wissenschaftlicher Arbeiten zu erfassen. Dadurch wird eine gezielte Selektion relevanter Quellen ermöglicht, während zugleich potenziell irreführende Ergebnisse aus der Stichwortsuche systematisch überprüft und ausgeschlossen werden können.

Vergleich der Kodierergebnisse zwischen Mensch und KI

Ein zentraler Aspekt der qualitativen Clustervalidierung ist der Vergleich zwischen menschlichen Kodierungen und KI-gestützten Inhaltsanalysen. Um die methodische Präzision beider Ansätze zu bewerten, wurden die Silhouette-Scores der jeweiligen Analysen berechnet. Die Ergebnisse zeigen deutliche Unterschiede in der Trennschärfe der Cluster.

**Vergleich der Silhouette-Scores: KI-gestützte Analyse vs. menschliche Kodierung**

Zur Überprüfung der methodischen Präzision und Trennschärfe von KI-gestützten Analysen im Vergleich zu menschlichen Kodierungen wurde die qualitative Clustervalidierung auf eine klassisch kodierte Studie von Kerman et al. [-@kerman_online_2024] angewendet. Ziel war es, die Clusterstruktur beider Verfahren zu vergleichen und Unterschiede in der methodischen Konsistenz zu identifizieren.

Die Analyse ergab, dass die KI-gestützte Analyse einen Silhouette-Score von 0.92 erreichte, während die menschliche Kodierung lediglich 0.62 betrug. Dies verdeutlicht die höhere methodische Präzision und Trennschärfe der KI-gestützten Analyse. Während die manuelle Kodierung stärkere Überschneidungen zwischen Kategorien aufwies, erzeugte die KI-gestützte Analyse klar abgegrenzte Clusterstrukturen mit geringerem inhaltlichem Überlapp.
Die Ergebnisse zeigen, dass KI-gestützte Inhaltsanalysen eine objektivere und methodisch konsistentere Alternative zur klassischen Kodierung darstellen können. Die qualitative Clustervalidierung bestätigt, dass menschliche Kodierungsprozesse anfällig für subjektive Einflüsse sind und eine systematische Überprüfung erfordern. Die methodische Stabilität der KI-Analyse verdeutlicht die Notwendigkeit, qualitative Inhaltsanalysen durch datenbasierte Validierung zu ergänzen.


Um die methodische Präzision und Trennschärfe von KI-gestützten Analysen im Vergleich zu menschlichen Kodierungen zu überprüfen, wurde die qualitative Clustervalidierung erneut auf die Studie von Kerman et al. [-@kerman_online_2024] angewendet. Absicht war der Vergleich der Clusterstruktur zwischen menschlicher Kodierung mit der KI-gestützten Analyse und mögliche Unterschiede in der methodischen Konsistenz zu identifizieren.

Die Analyse ergab, dass die KI-gestützte Analyse einen Silhouette-Score von 0.92 erreichte, während die menschliche Kodierung einen Wert von 0.62 aufwies. Dies zeigt, dass die KI-gestützte Analyse eine deutlich höhere Trennschärfe aufweist und methodisch konsistenter arbeitet. Während die menschlichen Kodierungen inhaltliche Überschneidungen aufwiesen und Kategorien nicht immer klar voneinander abgrenzbar waren, erzeugte die KI-gestützte Analyse präzisere Clusterstrukturen mit geringeren inhaltlichen Überlappungen (vgl. . 

Dieses Ergebnis bestätigt, dass KI-gestützte Inhaltsanalysen methodisch präziser sein können als menschliche Kodierungen. Die qualitative Clustervalidierung zeigt auf, dass menschliche Kodierungsprozesse eine größere Subjektivität aufweisen und daher eine systematische Überprüfung erforderlich ist. Die methodische Stabilität der KI-Analyse verdeutlicht, dass eine datengestützte Validierung menschlicher Kodierungen notwendig ist, um eine methodisch fundierte qualitative Inhaltsanalyse zu gewährleisten.

Die KI-gestützte Analyse erreichte einen Silhouette-Score von 0.92, während die menschliche Kodierung nur einen Wert von 0.62 aufwies. Dies bestätigt, dass KI-gestützte Inhaltsanalysen eine höhere methodische Präzision und Trennschärfe aufweisen als klassische manuelle Kodierungen. Die qualitative Clustervalidierung wurde auf eine klassisch kodierte Studie von Kerman et al. angewendet, um deren methodische Trennschärfe systematisch zu überprüfen und mit einer KI-gestützten Analyse zu vergleichen. Dabei zeigte sich, dass die KI-Analyse klarere Clusterstrukturen erzeugte, während die menschliche Kodierung stärkere Überschneidungen zwischen den Kategorien aufwies. Ein hoher Silhouette-Score deutet auf eine starke Gruppierung der Datenpunkte hin, während ein niedrigerer Wert auf Überlappungen zwischen den Kategorien hindeutet.

**Testansätze**

Ein wesentlicher Bestandteil der qualitativen Clustervalidierung ist die systematische Überprüfung der Analyseergebnisse anhand definierter Testansätze. Zunächst erfolgt eine automatische Kodierung, bei der untersucht wird, ob die Methode relevante Konzepte aus dem Text extrahiert und korrekt zuordnet. Anschließend wird die extrahierte Struktur mit der ursprünglichen Kodierung in der Studie verglichen, um mögliche Abweichungen oder Übereinstimmungen zu identifizieren.

Ein weiterer Schritt ist die Clusterbildung mit $k$-Means, um zu prüfen, ob sich inhaltlich sinnvolle Cluster innerhalb der Daten ergeben. Diese werden mit den thematischen Schwerpunkten der Studie abgeglichen, um zu evaluieren, inwiefern die identifizierten Cluster mit etablierten Forschungsstrukturen übereinstimmen.

Zur Stabilitätsprüfung der Analyse wird der Silhouette-Score berechnet, wobei die Clustervalidierungmehrfach durchgeführt wird. Dadurch kann überprüft werden, ob sich die ermittelten Cluster über verschiedene Durchläufe hinweg stabil zeigen oder ob signifikante Schwankungen auftreten. Dies dient als Maß für die methodische Konsistenz der Validierung.

Ein abschließender Vergleich erfolgte durch die Anwendung der qualitativen Clustervalidierung auf die klassisch kodierte Studie von Kerman et al. Dabei wurde analysiert, inwiefern die von Menschen kodierten Kategorien eine ähnlich klare Trennung aufweisen wie die KI-generierten Cluster. Die Ergebnisse zeigen, dass die Clustervalidierung eine objektive Bewertung der bestehenden Kodierung ermöglicht und methodische Schwächen in der menschlichen Kategorisierung sichtbar machen kann.

**Ergänzung zu ATLASeite ti und $k$-Means**

In der Diskussion zur methodischen Validierung wurde auch die Möglichkeit betrachtet, klassische Inhaltsanalyse-Tools wie ATLASeite ti 9 oder NVivo für die Analyse KI-generierter Kodierungen einzusetzen. Dabei zeigte sich jedoch, dass diese Werkzeuge primär für die Unterstützung menschlicher Kodierungsprozesse konzipiert sind und keine geeignete Methodik zur objektiven Validierung von Clustern bieten. Die qualitative Clustervalidierung verfolgt hingegen einen anderen Ansatz: Sie nutzt Algorithmen wie $k$-Means nicht zur explorativen Clusterbildung, sondern zur quantitativen Prüfung der methodischen Konsistenz bereits vorhandener Kodierungen. Diese Unterscheidung ist zentral, da die qualitative Clustervalidierung nicht als Konkurrenz zu klassischen Inhaltsanalyseverfahren betrachtet werden sollte, sondern als eine ergänzende Methode zur Überprüfung der Trennschärfe und methodischen Stabilität kodierter Daten.

**Kritische Einordnung bestehender Literatur**

In der aktuellen wissenschaftlichen Debatte über die Nutzung von KI in akademischen Kontexten sind zahlreiche Publikationen zu finden, die vor den potenziellen Risiken von KI-generierten Inhalten warnen. Arbeiten wie die von Biswas [-@biswas_chatgpt_2023], Van Niekerk et al. [-@van_niekerk_addressing_2025], Storey [-@storey_ai_2023] und Parker et al. [-@parker_negotiating_2024] thematisieren ethische Implikationen, wissenschaftliche Integrität und Herausforderungen im Peer-Review-Prozess. Dabei bleibt jedoch ein entscheidender Aspekt unbeachtet: Bislang existiert keine fundierte empirische Methode zur systematischen Überprüfung der Qualität von KI-generierten wissenschaftlichen Inhalten. Die genannten Studien diskutieren Risiken und Problematiken, liefern dabei keine methodische Grundlage für eine objektive Bewertung der wissenschaftlichen Qualität von KI-generierten Texten. Ein zentrales Defizit dieser Arbeiten besteht in der fehlenden empirischen Prüfung von KI-gestützten wissenschaftlichen Texten. Während argumentiert wird, dass KI-generierte Inhalte problematisch seien, fehlen systematische Vergleiche zwischen KI- und menschlich erstellten Texten sowie methodische Verfahren zur Überprüfung der Trennschärfe von KI-gestützten Analysen. Diese Arbeiten verbleiben weitgehend auf der deskriptiven Ebene und bieten keine quantitativen oder qualitativen Metriken zur Messung der methodischen Präzision von KI-generierten Inhalten.

**Bedeutung für die qualitative Forschung**

Die Ergebnisse zeigen, dass die qualitative Clustervalidierung eine objektive Bewertung von Kodierungen ermöglicht und methodische Schwächen sichtbar machen kann. Dies legt nahe, dass KI-gestützte Inhaltsanalysen eine präzisere Ergänzung zur klassischen qualitativen Kodierung darstellen können. Insbesondere in groß angelegten Studien mit umfangreichen Textkorpora könnten KI-basierte Verfahren eine erhebliche methodische Verbesserung ermöglichen.

Gleichzeitig bleibt zu beachten, dass menschliche Kodierungen theoretische Konzepte und interpretative Nuancen einbeziehen können, die über rein datenbasierte Analysen hinausgehen. Diese Erkenntnisse unterstreichen das Potenzial der qualitativen Clustervalidierung als standardisiertes Verfahren zur Überprüfung methodischer Trennschärfe. Langfristig könnte sie als ergänzende Methode zur Qualitätssicherung klassischer Kodierungsverfahren etabliert werden. In der qualitativen Forschung könnte daher ein hybrider Ansatz sinnvoll sein, bei dem KI-gestützte Analysen zur Strukturierung und Validierung menschlicher Kodierungen eingesetzt werden.

### 4.3.3 Mehrdimensional-analytische Clustervalidierung (mdaCV) {#sec:4-3-3}

Im Zuge der systematischen Literaturarbeit wurde die statistische Clusteranalyse, eher zufällig als potenzielle Erweiterung der qualitativen Analyse in Betracht gezogen (Kapitel 4.3.1). Die Anwendung des $k$-Means-Algorithmus auf einen bereits deduktiv strukturierten Quellenkorpus erschien als vielversprechender Zugang zur Identifikation verborgener Muster oder nicht explizit abgebildeter Strukturen. Überraschenderweise blieben jedoch neue Erkenntnisse aus, da die Clustervalidierung weitgehend die bestehenden semantischen Erkenntnisse bestätigte. Diese zunächst irritierende Stabilität erwies sich im weiteren Verlauf als methodisch hochbedeutsam. Die Tatsache, dass ein klassisch induktiv genutzter Algorithmus ein deduktiv geschaffenes Ordnungssystem reproduzierte, verweist auf eine inhärente Validierung der Ausgangsstruktur. Erst mit zeitlichem Abstand wurde deutlich, dass sich hier eine neue methodische Perspektive eröffnet, d.h. die Möglichkeit, qualitative Strukturierungslogiken algorithmisch zu überprüfen.

Aus dieser Beobachtung entwickelte sich schrittweise die mehrdimensional-analytische Clustervalidierung (mdaCV). Ein Verfahren, das qualitative Strukturierung, algorithmische Clusterdetektion und visuelle Repräsentation in einem konsistenten Validierungsprozess verbindet. Dabei wird ein deduktiv formulierter semantischer Raum entlang inhaltlich begründeter Dimensionen (z.B. Kategorien, Forschungsfragen, Schlagworte) aufgespannt. Die Positionierung der Datenpunkte erfolgt entlang dieser Achsen, die Clusterbildung erfolgt mit dem $k$-Means-Algorithmus, die Qualität der Trennung wird über den Silhouette-Score erfasst ([@rousseeuw_silhouettes_1987]).

Erst in einem späteren Entwicklungsschritt wurde deutlich, dass diese Vorgehensweise nicht nur für menschlich kodierte, sondern auch für KI-generierte Analysen geeignet ist. Durch die Anwendung auf Testdatensätze, die  swohl real, manipuliert als auch zufällig gestaltet wurden, konnte nachgewiesen werden, dass die mdaCV zwischen kohärenten, rauschhaften und künstlich homogenisierten Datenstrukturen zuverlässig differenziert. Die methodische Implementierung wurde versioniert dokumentiert und ist unter https://github.com/jochen-hanisch/charite-promotion öffentlich einsehbar. Dort finden sich sowohl der vollständige Datensatz mit Testvarianten (Real-, Zufalls- und manipulierte Daten) als auch die korrespondierenden Python-Skripte (`analyse_korrelation.py`, `analyse_netzwerk.py`) sowie ein angepasstes .gitignore, zur Sicherstellung, dass keine personenbezogenen oder bibliographisch geschützten Inhalte öffentlich sichtbar sind.

Die Methode wurde nicht abstrakt konzipiert, sondern emergierte aus forschungspraktischen Überlegungen, iterativen Rückkopplungen und der Notwendigkeit, große Datenmengen zugleich strukturiert, nachvollziehbar und validierbar zu analysieren. Die theoretische Herleitung basiert u. a. auf Arbeiten zur Stabilität des $k$-Means-Algorithmus [@rakhlin_stability_nodate, Kapitel 5], zur Struktur von Merkmalsräumen [@mavroeidis_novel_2011, Kapitel 3] sowie zur algorithmischen Modellierung semantischer Nähe durch Vektorraummodelle [@mikolov_efficient_2013, Kapitel 2]. Die mdaCV verbindet somit deduktive Theoriegeleitetheit mit datenbasierter Validierungslogik. Ein methodisches Hybridmodell, das qualitative und quantitative Paradigmen nicht nur überbrückt, sondern integrativ zusammenführt.

Die mdaCV ist ein Verfahren zur Validierung von Kodierungsstrukturen in qualitativ vorstrukturierten Datenräumen. Dieses Verfahren basiert auf einem dreidimensionalen semantischen Raum, in dem Datenpunkte entlang deduktiv definierter Achsen (z.B. Kategorien, Forschungsfragen, Schlagworte) positioniert und anschließend mittels algorithmischer Clustervalidierung überprüft werden. Dabei kombiniert das Verfahren inhaltlich fundierte Dimensionen mit statistischen Bewertungsverfahren wie dem Silhouette-Score ([@rousseeuw_silhouettes_1987 Seite 59, 61]), um die Trennschärfe und Kohärenz der Clusterbildung zu bewerten.

Die methodische Herleitung fußt auf drei zentralen Komponenten:

1. Deduktive Strukturierung des semantischen Raums : Aufbauend auf theoretisch oder empirisch begründeten Dimensionen erfolgt eine systematische Vorstrukturierung des Datenraums [@baur_datenaufbereitung_2022; @baur_qualitative_2022; @baur_qualitative_2022]. Diese Dimensionen bspw. Kategorien, Disziplinen oder thematische Schlagworte, definieren die Achsen des Raums und ermöglichen die strukturierte Positionierung der Daten.
2. Um die semantische Struktur der Daten algorithmisch analysierbar zu machen, werden begriffliche Relationen in numerische Vektoren überführt. Die semantische Nähe zwischen Datenpunkten entspricht dabei ihrer geometrischen Nähe im Vektorraum. Diese Transformation bildet die Grundlage für distanzbasierte Verfahren wie die Clustervalidierung. Konzepte wie CBOW und Skip-gram (Mikolov et al., 2013, Kapitel 6) zeigen, dass auch mit vergleichsweise einfachen Modellarchitekturen hochdimensionale, semantisch präzise Repräsentationen berechnet werden können [@mikolov_efficient_2013]. Dies ermöglicht die effiziente Verarbeitung großer Korpora und bildet die konzeptionelle Basis für die Vektorraummodellierung in der mdaCV.
3. Statistische Validierung mittels $k$-Means-Algorithmus: Die deduktiv vorstrukturierten Daten werden dem $k$-Means-Verfahren unterzogen. Die zentrale mathematische Formulierung basiert auf der Minimierung der quadrierten Distanzen innerhalb der Cluster (Pérez-Ortega et al., 2020, Seite 5) [@sud_k-means_2020, Seite 5]. Die Wahl der Anzahl der Cluster $k$ erfolgt theoriegeleitet oder wird durch Metriken wie den Silhouette-Score empirisch justiert. Die Sensitivität des $k$-Means-Algorithmus gegenüber strukturellen Varianzen wird dabei bewusst genutzt, um die methodische Konsistenz der Vorstrukturierung zu evaluieren [@rakhlin_stability_nodate].

Diese Kombination aus inhaltlicher Fundierung, geometrischer Modellierung und algorithmischer Validierung begründet die mdaCV als eigenständiges methodisches Verfahren. Sie wurde im Verlauf der Dissertation iterativ verfeinert, insbesondere durch Tests mit realen, manipulierten und zufälligen Datensätzen, um ihre Robustheit gegenüber Rauschelementen und ihre Fähigkeit zur Differenzierung inhaltlicher Kohärenz nachzuweisen (vgl. [@sud_k-means_2020, Seite 5, Punkt 4]). Damit stellt die mdaCV keine bloße Kombination bestehender Verfahren dar, sondern ein transmethodisches Integrationsmodell, das qualitative Kategoriensysteme auf algorithmisch validierbare Weise überprüfbar macht - ein Beitrag zur Qualitätssicherung, Reproduzierbarkeit und epistemischen Transparenz in der qualitativen Bildungsforschung.
Die mehrdimensional-analytische Clustervalidierung begleitete nicht nur den Analyseprozess im engeren Sinne, sondern wurde über den gesamten Promotionszeitraum hinweg als sensible, seismografisch wirkende Dauermessung eingesetzt. Die jeweiligen Messpunkte wurden nach gezielten Veränderungen am Suchbegriffkorpus vorgenommen und erlauben eine fortlaufende Rückmeldung über die semantische Konsistenz des Quellenraums, wobei die Anzahl der Cluster dauerhaft mit $n = 4$ beibehalten wurde.

Im Rahmen einer Analyse (Achsen: Suchbegriff, Kategorie, Forschungsfrage) wärhend des Forschungsprozesses wurde der Korpus beispielsweise zunächst auf $n = 3502$ Quellen bereinigt, indem bestimmte Dokumentgattungen (etwa Manuskripte oder unspezifische Vorabfassungen) ausgeschlossen wurden. Infolge dieser Kuration stieg der Silhouette-Score von $0.964$ auf $0.9751$. Diese Differenz ist nicht als bloße numerische Verbesserung zu verstehen, sondern als qualitatives Emergenzphänomen. Nach Einbezug der o.a. Herleitung, wirkt jede Bereinigung in einem semantisch hochdimensionalen Raum potenziell in alle Richtungen. Der Erkenntniswert liegt somit weniger in der absoluten Score-Steigerung, sondern in der damit verbundenen epistemischen Schärfung, die sich durch den Ausschluss semantischer Rauschelemente ergibt. Hier demonstriert die Analyse exemplarisch, wie sich durch dreidimensional deduktive Validierung eine strukturell kohärente Quellenarchitektur rekonstruieren lässt.

Nach erneuter Einbindung der zuvor ausgeschlossenen Konferenzbände stieg die Anzahl der analysierten Quellen auf $n = 3572$. Überraschenderweise blieb der Silhouette-Score mit $0.9754$ nicht nur stabil, sondern übertraf den vorherigen Wert sogar leicht. Dieses Ergebnis legt nahe, dass die dreidimensionale deduktive Validierung hinreichend robust ist, um auch heterogene Dokumenttypen kohärent zu integrieren. Der ursprünglich befürchtete semantische Rausch-Effekt durch Konferenzbeiträge trat nicht ein; vielmehr scheint die zunehmende Datenfülle eine semantische Verdichtung zu bewirken. Das Cluster-Modell reagiert dabei nicht empfindlich, sondern resilient-emergent auf Datenerweiterung.

Die Beobachtungen von Veränderungen innerhalb der mehrdimensional-analytische Clustervalidierungsind insbesondere im Grenzbereich zwischen Systemstabilität und kategorialer Modifikation aufschlussreich. In einem weiterem Durchgang wurde der Eintragstyp Buchteil gezielt untersucht. Dabei wurde der Datensatz minimal um einen Eintrag reduziert (nun $n = 3571$), woraufhin sich der Silhouette-Score um $-0.001$ veränderte. Diese Differenz mag numerisch klein erscheinen, ist jedoch im Kontext eines Scores über $0.97$ hochrelevant. In diesem Bereich deutet bereits eine Veränderung in der dritten Nachkommastelle auf strukturelle Anpassungen im Clustermodell hin, etwa durch leicht verschobene Clusterzentren oder veränderte Einpassung eines Einzelbeitrags.
Diese hier exemplarisch angedeutete Sensitivität ist Ausdruck der hohen Auflösung und Differenzierungsfähigkeit des Modells. Im Gegensatz zu vielen anderen Clustering-Ansätzen, die bei kleinen Eingriffen stark „springen“, reagiert dieses System kontinuierlich und rückmeldungsfähig. Der Eintragstyp Buchteil könnte beispielhaft eine inhärent variablere semantische Positionierung besitzen, etwa durch seine Funktion als Vorwort, methodischer Einschub oder Randthema. Auch eine Überrepräsentation bestimmter Werke kann potenziell zu Verzerrungen führen. Die gezielte Analyse solcher Subtypen eröffnet Möglichkeiten für weiterführende Fragestellungen: Wie viele Buchteile stammen aus dem gleichen Werk? Welche Achsendimensionen beeinflussen ihre Clusterzuordnung? Und inwieweit führt das gezielte Entfernen einzelner Elemente zu strukturellen Verschiebungen im Modell?
Eine Veränderung von bspw. $0.001$ bei konstantem Stichprobenumfang und stabiler $k$-Means-Architektur stellt eine reale, systemisch interpretierbare Verschiebung dar. Das System reagiert feinfühlig, d.h. auf Einzelbeiträge und dokumentiert deren Auswirkungen auf die Gesamtstruktur. Daraus ergeben sich potenzielle Analysepfade zur Erforschung mikrostruktureller Dynamiken innerhalb epistemisch strukturierter Clusterräume. Wie Tabelle 5 darstellt, überlagern sich nicht nur qualitative und quantitative Paradigmen, sondern verzahnen sich strukturell.
Tabelle 6: Strukturelle Paradigmen-Überlagerung bei Clusteranalysen
Quantitativ	Qualitativ

<<<Abbildungen einfügen>>>

Silhouette-Score als Gütemaß	Deduktive Kategorienstruktur
Clusterdichte und Trennschärfe	Theoriegeleitete Semantikachsen

$k$-Means als algorithmischer Kern	Vorstrukturierung durch Forschungsperspektiven
Die Darstellung verdeutlicht, wie sich deduktive, theoriegeleitete Kategorien mit algorithmischen, quantitativ validierbaren Verfahren, etwa dem $k$-Means-Algorithmus und dem Silhouette-Score, strukturell verzahnen. Diese methodische Komplementarität ist zentral für die mehrdimensional-analystische Clustervalidierung (mdaCV) und ermöglicht die gleichzeitige Berücksichtigung epistemischer Tiefenstruktur und formaler Trennschärfe.
Besonders hervorzuheben ist dabei, dass die methodische Verzahnung nicht nur eine Erweiterung quantitativer Validierungsmaßstäbe bedeutet, sondern auch die Öffnung für neue, integrative Bewertungsdimensionen. Während die klassische Clusterbewertung meist auf einzelne numerische Kennzahlen fokussiert, rückt der mdaCV-Ansatz die Notwendigkeit einer umfassenderen Güteprüfung ins Zentrum, bei der neben der formalen Trennschärfe auch die inhaltliche Erfassungstiefe und Vollständigkeit der Daten eine Rolle spielt. Damit wird der Blick für latente Verlustrisiken geschärft, die rein metrische Metriken bislang ausblenden.

**Epistemische Verlustfunktion als heuristisches Integritätsmaß**

Im Kontext der mehrdimensional-analytischen Clustervalidierung wird üblicherweise der Silhouette-Score als zentrales Maß zur Beurteilung der Clusterdifferenzierung genutzt (i.A.a. Rousseeuw [-@rousseeuw_silhouettes_1987]). Dieser Wert allein erfasst jedoch lediglich die geometrische Separierbarkeit der Cluster im Vektorraum. Was bislang fehlt, ist ein zusammengesetztes Maß, das sowohl die strukturelle Kohärenz (Silhouette) als auch die semantische Vollständigkeit (Datenintegrität) einer Analyse widerspiegelt. Im Rahmen dieser Dissertation wurde daher eine epistemische Verlustfunktion $\varepsilon$ eingeführt, die beide Dimensionen in einem einzigen heuristischen Indikator vereint. Ziel dieses Verfahrens ist die Modellierung eines skalierbaren Integritätsmaßes, welches sowohl den Grad der Clusterdifferenzierung als auch den Umfang erfasster Quellen berücksichtigt. Die Funktion kann damit als Überwachungsgröße für Datenverarbeitungsläufe herangezogen werden und kritische Abweichungen sichtbar machen, die sich nicht allein über Silhouette- oder Dokumentenzahl abbilden lassen. Die epistemische Verlustfunktion wird von den beiden Größen Clusterdifferenzierungsleistung, gemessen über den Silhouette-Score, und Datenvollständigkeit, gemessen über das Verhältnis zwischen intendierter und tatsächlich verarbeiteter Quellenzahl. Die epistemische Verlustfunktion $\varepsilon$ wird wie folgt definiert:

Formel zur Definition der Verlustfunktion:

$$
\varepsilon = (1 - S) + \frac{n_{\text{Soll}} - n_{\text{Ist}}}{n_{\text{Soll}}}
$$ {#eq:verlust}

Diese additive Formulierung bringt zwei unterschiedliche Validitätsaspekte auf eine gemeinsame Skala:
    
- Struktureller Verlust, formuliert als $(1-S)$, wobei $S$ den Silhouette-Score repräsentiert. Diese Größe misst die Abweichung vom optimalen Clusteringwert $S=1$. Je niedriger der Silhouette-Score, desto größer ist der Verlust an struktureller Trennschärfe und Clusterkohärenz.
- Datenverlust, formuliert als $\frac{n_{\text{Soll}} - n_{\text{Ist}}}{n_{\text{Soll}}}$. Dieser Term beschreibt den relativen Anteil an Quellen, die nicht in die Analyse einflossen. Je höher der Wert, desto größer ist die epistemische Lücke im analysierten Datenkorpus.
- Beide Komponenten sind dimensionslos, additiv kombinierbar und liegen im Werteberich $W=[0,2]$. Die resultierende Funktion $\varepsilon$ gibt somit eine Gesamtverlustschätzung für die epistemische Integrität eines Analyseverfahrens.

Angenommen, ein Analyse-Korpus umfasst $n_{\text{Soll}} = 3585$ Einträge, in die Clustervalidierung gingen $n_{\text{Ist}} = 3583$ Quellen ein. Der ermittelte Silhouette-Score beträgt $S = 0,9754$. Dann ergibt sich:

$$
\varepsilon = (1 - 0,9754) + \frac{2}{3585} \approx 0,0246 + 0,000558 \approx 0,0252
$$

Die epistemische Verlustfunktion liegt in diesem Fall mit $\approx 0,0252$ in einem sehr niedrigen Bereich. Sie zeigt, dass trotz kleiner Datenverluste und nicht perfekter Trennschärfe eine nahezu optimale Integrität erreicht wurde. Damit bietet somit $\varepsilon$ eine differenzierte Perspektive auf die Validität einer Analyse und eignet sich insbesondere:

- zur Qualitätssicherung von Analysepipelines (z. B. automatische Literaturanalysen, KI-generierte Korpora),
- zum Vergleich unterschiedlicher Datenverarbeitungen (z. B. real vs. manipuliert vs. zufällig) sowie
- als metawissenschaftliche Monitoring-Größe in dynamischen Forschungsumgebungen.

Der Nutzen dieses Maßes liegt nicht in seiner absoluten Exaktheit, sondern in der epistemischen Sensibilität. Schon kleinste Abweichungen vom Ideal (Silhouette < 1 oder Datenlücken) werden sichtbar gemacht und können reflektiert werden, woraus eine neue Form der kontinuierlichen Gültigkeitsüberwachung in datenintensiven Forschungsprozessen entsteht. Die hier eingeführte epistemische Verlustfunktion $\varepsilon$ stellt ein heuristisches und gleichzeitig methodisch begründetes Integritätsmaß dar, das den Anspruch der mdaCV auf Verknüpfung qualitativer und quantitativer Güteprinzipien konsequent weiterführt. Sie ist anschlussfähig für weitere Forschungsdesigns, maschinelle Analysen und metawissenschaftliche Validitätsdiskurse.

## 4.4 Simulationsgestützte Modellierung der Kompetenzentwicklung {#sec:4-4}

## 4.5 Reflexion der Methode {#sec:4-5}

Die kritische Methodenreflexion hat den Zweck, die eigene Arbeitsweise transparent, nachvollziehbar und anhand des wissenschaftlichen Qualitätskriteriums „Methodische Strenge“ [@doring_forschungsmethoden_2023, Seite 89-90] beurteilbar zu machen. Inwiefern diese Arbeit die Anforderungen an eine methodisch saubere, nachvollziehbare und theoriegeleitete Forschung erfüllt, ist in diesem Kapitel zu klären.

Als Herleitungsgrundlage kann ein systemisch-konstruktivistisches Verständnis von Erkenntnis angesetzt werden, das mit bewährten Evaluationsmodellen (z. B. dem CIPP-Modell nach Stufflebeam in [@hanisch_nachhaltiges_2017, Kapitel  3.1]) sowie analytischen Verfahren wie Korrelations- und deduktiven Clusteranalysen verbunden wird. Diese Kombination ist weder beliebig noch additiv, sondern strukturell aufeinander bezogen und somit theoriekompatibel. Die Auswahl der Methoden ergibt sich aus der forschungsfragengeleiteten Logik. Sie folgt keiner Paradigmentreue, sondern einem funktionalen Verständnis von Methodeneinsatz und hat zur Folge, dass qualitative und quantitative Verfahren entlang der FU dort eingesetzt werden, wo sie zur Bearbeitung beitragen. Die theoretischen Begriffe (z. B. Kompetenz, Selbstorganisation, Nachhaltigkeit) werden auf konkrete Analyseebenen übertragen, etwa über Prädiktorvariablen (z. B. PV1a-PV3 bei Hanisch [-@hanisch_nachhaltiges_2017, Kapitel  3.4]) oder KI-gestützte Analysen . Sämtliche Analyseprozesse, von der Auswahl der Quellen, über die Generierung und Anwendung der Prompts, bis hin zur Auswertung und Rückführung in die FU, sind dokumentiert, versioniert und theoretisch hergeleitet. Die Struktur folgt einer nachvollziehenden analytischen Logik, die von der FU über die erste KI-gestützte Analyse bis zur Metaebene mit Clusterauswertungen übergeht. Als kuratierende Hilfsmittel unterstützen digitale Werkzeuge, unter deren Verwendung das Literatur- und Notizmanagement (Zotero), die Versionierungen (Gitea), sowie die statistischen Berechnungen und Visualisierungen (Python) durchgeführt werden konnten. Diese Kombination von Methoden und Werkzeugen gewährleistet sowohl Reproduzierbarkeit als auch in sich Konsistenz.

Bereits in der Zusammenstellung der Analyseeinheiten werden bewusste Entscheidungen getroffen, zum Beispiel zur Nichtberücksichtigung von Masterarbeiten und reiner „grauer Literatur“ in bestimmten Clusteranalysen. Diese werden nicht nur transparent dargestellt, sondern auch theoriebezogen begründet. Dadurch erhöht sich die Validität der Aussagen.

Ein wesentlicher Bestandteil des methodischen Vorgehens ist die fortlaufende Selbstprüfung und Justierung. Dazu gehören die Prüfung der Wirksamkeit der Prompts, die Diskussion der Silhouette-Werte zur Clustertrennschärfe, aber auch die bewusste Unterscheidung zwischen Analysen 1. Ordnung (einzelne Quelle) und Analysen 2. Ordnung (übergreifende Auswertung, Rückführung auf die FU).
Mein methodisches Vorgehen erfüllt, trotz seiner systemisch-flexiblen Struktur, zentrale Anforderungen wissenschaftlicher Strenge. Die Methoden sind theoriebasiert, nachvollziehbar, funktional gewählt und systematisch eingesetzt. Gleichzeitig erweitere ich die bestehende Methodendiskussion durch den reflektierten Einsatz generativer KI als epistemisches Werkzeug und durch die Integration klassischer Evaluationsverfahren in ein offenes, komplexitätssensibles Design.

Diese Vorgehensweise ist nicht nur methodisch tragfähig, sondern auch ein konkreter Beitrag zur Weiterentwicklung digital-epistemischer Forschung in Bildungssettings.

Selbstverständlich muss im Sinne der wissenschaftlichen Redlichkeit ([@doring_forschungsmethoden_2023, Seite 130-131]) und in Anbetracht der aktuellen kritischen Haltung gegenüber generativen bzw. künstlichen Intelligenzen das hier gewählte methodische Vorgehen nicht nur dargelegt, sondern im besonderen Maße nachvollziehbar erläutert werden. Als Grund für diese Erklärung kann angeführt werden, dass die wissenschaftliche Eigenleistung infrage gestellt werden kann, wenn die Analysen GPT-basiert durchgeführt werden. Das methodische Vorgehen, d. h. die Durchführung inhaltsanalytischer Einzelanalysen mithilfe von GPT und deren anschließende Zusammenführung durch eine deduktive, auf Forschungsunterfragen ausgerichtete Cluster- und Metaanalyse, stellt eine eigenständige wissenschaftliche Leistung dar. Diese kann durch folgende Begründungslogik belegt werden:

- Selbständige Definition erkenntnisleitender Kategorien: Die zugrunde liegenden Kategorien und Kodierungen (wie z.B. „Akzeptanz“, „Nützlichkeit“, „Effekt“, „Gestaltung“) wurden aus den Forschungsunterfragen eigenständig abgeleitet. Diese Kategorien sind als deduktive Filter anzusehen, welche die Ausrichtung und Vergleichbarkeit der GPT-gestützten Einzelanalysen ermöglichen. Ohne diese Struktur blieben die Ergebnisse der Analysen unsystematisch und nicht aggregierbar.
- Eigenständige wissenschaftliche Durchführung der Metaanalyse: Die Analysen führen zu keiner Aggregation klassischer Primärforschungsergebnisse, sondern werden zu semantisch strukturierten, vorbereiteten GPT-Einzelanalysen verdichtet. Diese enthalten bereits wissenschaftliche Extrakte, deren Struktur vorgegeben wird. In einem weiteren Schritt wird geprüft, ob die Ergebnisse im Hinblick auf die Forschungsunterfragen widerspruchsfrei, konsistent und saturiert sind. Strukturell entspricht dies einem theoriegeleiteten Validierungsschritt, wobei sowohl die analytischen Kategorien als auch die Aussagekraft der Analysen überprüft werden. 
- GPT als analytisches Werkzeug, nicht als Urheberschaft: GPT wird ausschließlich als analytisches Instrument eingesetzt, vergleichbar mit etablierten Softwarelösungen wie SPSS oder MaxQDA. Die Verantwortung für Struktur, Steuerung und Auswertung lagen zu jeder Zeit vollständig in der eigenen Hand. Die wissenschaftliche Eigenständigkeit resultiert somit nicht aus der Textgenerierung, sondern aus der theoretischen Fundierung und Auswertung der Ergebnisse.
- Geschlossenes System analytischer Selbstreferenz: Das Verfahren umfasst einen zyklischen Prozess: vom Theorierahmen über die empirische Anreicherung, die GPT-Analyse erster Ordnung, die Clusterdarstellung bis hin zur Rückbindung an die handlungsleitenden Forschungsunterfragen. Diese Form rekursiver Validierung stellt ein fortgeschrittenes und bislang wenig beschriebenes methodologisches Vorgehen dar.
- Beitrag zur wissenschaftstheoretischen Innovation: Das Vorgehen erfüllt Kriterien einer strengen Operationalisierung, methodischen Reflexion über Automatisierungsprozesse sowie einer systematischen Steuerung von KI als Analyse- und Verdichtungsinstrument. Damit entsteht ein möglicher methodologischer Prototyp für KI-unterstützte Metaforschung.

Infolgedessen liegt die wissenschaftliche Eigenleistung in der Strukturierung des Analyseprozesses, der Definition und Trennung der Ordnungsebenen (1. Ordnung: Analyse, 2. Ordnung: Bewertung), der methodologischen Fundierung (deduktiv und theoriebasiert) sowie in der reflexiven Kontrolle des Systems. Dieses Vorgehen ist originär, transparent dokumentiert und methodologisch innovativ.

Methodische Stärken

- Forschungsfragengeleiteter Ansatz mit systemischer Perspektive.
- Kombination klassischer Methoden (Literatur, Simulation, Eye-Tracking) mit innovativen Ansätzen (KI, Python).

Methodische Herausforderungen und Limitationen

- Herausforderungen:
  - Retrospektive Integration einiger Methoden.
  - Entwicklung eines eigenen Paradigmas zur Bearbeitung der Forschungsfragen.
- Limitationen:
  - Komplexität der Datenintegration.
  - Abhängigkeit von KI-Tools und Simulationen.
