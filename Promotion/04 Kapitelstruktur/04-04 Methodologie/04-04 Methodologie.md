# 4 Methodologie

Kapitel 4 beschreibt eine Methodik, die vollständig auf den Forschungsfragen basiert und durch systemtheoretische Prinzipien strukturiert ist. Die Kombination aus geplanten Methoden (z. B. Literaturanalyse, Eye-Tracking) und methodischen Erweiterungen (Python-Simulation) zeigt die Flexibilität und Innovationskraft der Arbeit.

## 4.1 Forschungsparadigma

Methodenkompetenz in den Human- und Sozialwissenschaften umfasst die Fähigkeit, empirische Studien nicht nur zu lesen und zu interpretieren, sondern diese auch selbstständig durchzuführen, um systematische und nachvollziehbare Erkenntnisse zu generieren. Dabei etablierten sich drei zentrale Forschungsparadigmen, die sich in ihren erkenntnistheoretischen Grundlagen und methodischen Logiken unterscheiden: (a) das quantitative Paradigma, basierend auf dem kritischen Realismus, (b) das qualitative Paradigma, verankert im Sozialkonstruktivismus, sowie (c) das Mixed-Methods-Paradigma, das im Pragmatismus wurzelt. Während das quantitative Paradigma einen linear-strukturierten Forschungsprozess postuliert, der auf zu überprüfende Hypothesen aufbaut, bildet das qualitative Paradigma einen zirkulären, wenig strukturierten Forschungsprozess mit offenen Forschungsfragen ab. Mit dem Mixed-Methods-Ansatz können komplexere, lineare sowie nichtlineare Vorhaben bearbeitet und mit verschiedenen Teilprozessen verbunden werden. Die Differenzierung der Paradigmen erweitert sich auch um die Rollenperspektive der Forschenden, die in Abhängigkeit vom untersuchten Forschungsgegenstand reflektiert werden muss. Entscheidend für die Wahl der Forschungslogik ist nicht, welche Daten (z.B. numerische oder textliche) vorliegen, sondern mit welchem Vorgehen die vorliegenden oder noch zu erzeugenden Daten methodisch zu bearbeiten sind. Das Begründungsgebot nimmt hierbei im wissenschaftlichen Arbeiten eine zentrale Stellung ein, da es die Wahl der Forschungslogik und die Bearbeitung von Daten methodisch legitimiert [@doring_empirische_2023, S. 4–5; @doring_wissenschaftstheoretische_2023, S. 32–33].

Methodisch herausfordernd in dieser Arbeit ist die Auflösung eines Dilemmas durch Verknüpfung der unterschiedlichen Facetten dieses bildungstheoretischen Forschungsvorhabens. Quantitative Daten, bspw. aus dem Eye-Tracking-Versuch und der begleitenden Umfrage, und qualitative Daten, bspw. die Ergebnisse aus der systematischen Literaturanalyse, sind miteinander in Bezug zu setzen, um übergeordnete Erkenntnisse zu generieren. Die Verwendung der beiden Paradigmen wird durch die Intention der Hauptforschungsfrage legitimiert, die Wissen um Muster und Regelmäßigkeiten im LMS erzeugen möchte. Insbesondere das vorgefundene Spannungsfeld von Subjektivität (Wahrnehmung der Akteure) und Objektivität (Kompetenzentwicklungssimulation) erfordert eine genauere methodische Betrachtung. Die sonst eher streng zugeordnete Forschungsmethodik, das quantitative Paradigma als deduktiv und das qualitative Paradigma als induktiv, greift hier zu kurz, da diese strikte Trennung die komplexe Wirkung des Forschungsgegenstands nicht abbilden kann [@reinders_uberblick_2022, S. 157].

Forschungstätigkeiten in Gesundheitskontexten stehen zudem vor der Herausforderung, unterschiedliche methodische Strömungen diverser Disziplinen für sich einzunehmen. Insbesondere der Umgang mit tradierten Forschungsparadigmen muss angesichts der Komplexität intradisziplinärer Forschungstätigkeiten beantwortet werden. Gerade Komplexität, vielfältige Disziplinen und unterschiedliche Ressourcen sind miteinander in Einklang zu bringen. Damit dies gelingt, können die jeweiligen Stärken und Chancen bisheriger Forschungsmethoden in einen neuen, interdisziplinären und generativen Kontext gestellt werden [@niederberger_forschungsmethoden_2021, S. 4–5].

Zwar verbindet das Mixed-Methods-Paradigma die beiden zuvor genannten Ansätze, steht jedoch in der Kritik, dass diese epistemologisch unvereinbar seien (z.B. Inkommensurabilitäts-These in Verbindung mit der Komplementaritäts-These) und daher methodisch fragil bleiben. Hinzu kommt, dass der Mixed-Methods-Ansatz häufig pragmatisch verwendet wird, wodurch quantitative und qualitative Verfahren unreflektiert nebeneinanderstehen. Auch die strikte Trennung der Paradigmen – das quantitative Paradigma als deduktiv und das qualitative Paradigma als induktiv – greift zu kurz, da sie die notwendige Integration von Regelmäßigkeiten (quantitative Ebene) und subjektiven Kontexten (qualitative Ebene) verhindert [@doring_wissenschaftstheoretische_2023, Kapitel 2].

Das hier beschriebene Forschungsvorhaben erfordert aufgrund seiner zirkulären Komplexität einen mehrdimensionalen Ansatz, der die bisherigen Ebenen systematisch aufeinander bezieht. Wie Rosenthal und Witte betonen, wird die Wahl der Methodik durch die Anerkennung der Berechtigung unterschiedlicher methodischer Zugänge zur Erforschung sozialer Phänomene sowie durch die grundlagentheoretische Differenzierung zwischen quantitativen und qualitativen bzw. interpretativen Forschungsansätzen beeinflusst [@mays_quanti_2020, S. 198–199]. In diesem Spannungsfeld versteht sich die vorliegende Arbeit als abstrakt-theoretische Grundlagenforschung. Damit soll der theoretische Anspruch eingelöst werden, methodische Vielfalt anzuerkennen und gleichzeitig eine systematische Integration der Perspektiven zu ermöglichen.

Die Auflösung des vorliegenden forschungsparadigmatischen Dilemmas erfolgt durch den Zugang zum Forschungsgegenstand über die konsequente Ableitung der Methoden aus den Forschungsfragen. Dieses Vorgehen ermöglicht nicht nur eine zielgerichtete Methodenauswahl, sondern auch eine Komplexitätsreduktion, die der Mehrdimensionalität des Forschungsgegenstandes gerecht wird und gleichzeitig die Stärken bestehender Methoden integriert.

### 4.1.1 Systemisch-forschungsfragengeleiteter Ansatz

Der systemische, forschungsfragengeleitete Ansatz dieser Arbeit fußt vollständig auf den Forschungsfragen FU1 bis FU7 (s. Kapitel 1.2.3), die aus dem Erkenntnisinteresse (s. Kapitel 1.1.1) und dem bestehenden LMS-Produkt (s. Kapitel 3) abgeleitet wurden. Diese Forschungsfragen strukturieren und leiten alle methodischen Entscheidungen und Analysen zur Bearbeitung. Die hier entwickelte Methodik, die den systemischen Ansatz mit der konsequenten Folgerung der Methoden aus den Forschungsfragen synthetisiert, ist in dieser spezifischen Form bisher nicht beschrieben. Damit werden systemtheoretische Prinzipien wie Interdependenz und Emergenz mit der gezielten Integration qualitativer und quantitativer Methoden verknüpft, um der zirkulären Komplexität des Forschungsgegenstandes gerecht zu werden.

Interdependenz bedeutet für die Methodologie des Forschungsprozesses, dass Forschungsfragen eng miteinander verknüpft sind und Wechselwirkungen zwischen qualitativen und quantitativen Daten erzeugen, wodurch die Mehrdimensionalität des Forschungsgegenstandes erfasst werden kann. Emergenz beschreibt ergänzend die Entstehung neuer Erkenntnisse [@bertalanffy_general_1968, S. 16, 103] durch die Verknüpfung von Ergebnissen aus Literaturanalysen, Simulationen und empirischen Untersuchungen wie Eye-Tracking-Analysen und Befragungen. Rückkopplung bedeutet in diesem Fall, dass Analyseergebnisse iterativ in die Methodik zurückfließen und die weiteren Schritte beeinflussen, wodurch der Forschungsprozess dynamisch bleibt und sich kontinuierlich anpasst.

Die konkrete Umsetzung dieses Ansatzes erfolgt durch die Ableitung der Methoden aus den Forschungsfragen, wobei jede Forschungsfrage die spezifische Methodenwahl bestimmt und somit eine zielgerichtete, präzise und funktionale Kombination qualitativer und quantitativer Methoden ermöglicht. Qualitative Literaturanalysen werden systematisch mit qualitativen Methoden wie Eye-Tracking-Analysen (z.B. Heatmaps) und quantitativen Befragungen kombiniert, um eine ganzheitliche Perspektive zu ermöglichen. Die eingesetzten Methoden werden dabei passgenau auf die jeweiligen Forschungsfragen abgestimmt und berücksichtigen sowohl subjektive Akteurswahrnehmungen als auch objektive Daten zur Mustererkennung.

Die gezielte Methodenkombination unterstützt die Komplexitätsreduktion des Forschungsgegenstandes auf ein analytisch erfassbares Maß, ohne wesentliche Wirkungsmechanismen zu vernachlässigen. Durch die iterative Rückkopplung und systemische Verknüpfung der Ergebnisse entstehen neue Einsichten, die bei isolierter Betrachtung der Methoden verborgen bleiben würden. Dieser innovative Ansatz erweitert bestehende methodische Ansätze und schafft einen neuen Rahmen, der sowohl Offenheit als auch strukturelle Präzision ermöglicht.

Methodische Konsequenzen der Forschungsfragen

- Die Forschungsfragen bestimmten:
  - Auswahl und Strukturierung der Literatur.
  - Entwicklung von Kategorien und Schlagworten zur thematischen Verknüpfung.
  - Kombination und Anpassung klassischer Methoden.
- **Begründung**:
  - Die Komplexität des digitalen Bildungsraums erforderte eine Methodenkombination, um die Forschungsfragen adäquat zu beantworten.

Forschungsdesign (ca. 3–4 Seiten)

Dynamischer, zirkulär-itterativer Forschungsprozess

Der Forschungsprozess wurde iterativ gestaltet. Neue Erkenntnisse wurden kontinuierlich in die nächsten Arbeitsschritte integriert, beispielsweise als später eine Python-Simulation eingebaut wurde. Insgesamt umfasste das Projekt sechs Phasen. Zunächst erfolgte die Planungsphase, in der Forschungsfragen formuliert, ein Exposee erstellt und die Methodik festgelegt wurden. Daran schloss sich die Recherche an; hierbei analysierte man Literatur mit Hilfe von KI, sammelte relevante Quellen und kategorisierte diese. In der Erhebungsphase wurden eine Umfrage zu digitalen Kompetenzen sowie Eye-Tracking-Analysen durchgeführt. Anschließend folgte die Modellierung, bei der eine Python-Simulation entwickelt und deren Ergebnisse miteinander verknüpft wurden. Die Schreibphase beinhaltete das Verfassen von Theorie, Methodik und Ergebnissen sowie das Einholen von Rückmeldungen zur Fragestellung. Abschließend wurden im Rahmen der Endredaktion die finale Einreichung und die Vorbereitung der Publikation vorgenommen.

Forschungsunterfragen und ihre Methoden

```{=latex}
\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Tabellarische Darstellung der Forschungsunterfragen (FU) und Methoden}
\label{tab:methoden_FU}
{\fontsize{8}{10}\selectfont
\begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}m{3cm}|X|}
\hline
\textbf{Forschungsunterfrage} & \textbf{Methoden} \\ \hline
\textbf{FU1: Akzeptanz und Nützlichkeit} & Systematische Literaturanalyse, KI-gestützte Analyse zur Untersuchung der Benutzerfreundlichkeit und Nützlichkeit von LMS. \\ \hline
\textbf{FU2a: Effekt auf Lernende} & Monte-Carlo-Simulation und 3D-Kompetenzmessmodell zur Modellierung der Kompetenzentwicklung und Unsicherheiten im Lernprozess. \\ \hline
\textbf{FU2b: Effekt auf Lehrende} & Umfrage zur digitalen Kompetenz von Lehrenden und ergänzende Literaturanalyse zur Erfassung der Herausforderungen und Bedürfnisse. \\ \hline
\textbf{FU3: Didaktische und technologische Merkmale} & Literaturanalyse, Eye-Tracking-Experiment und systemische Analyse zur Untersuchung der didaktischen und technischen Gestaltung von LMS. \\ \hline
\textbf{FU4a: Bildungswissenschaftliche Mechanismen} & Literaturanalyse zu bildungswissenschaftlichen Modellen und Mechanismen. \\ \hline
\textbf{FU4b: Technisch-gestalterische Mechanismen} & Eye-Tracking-Experiment mit begleitender Umfrage zur Analyse der Wahrnehmung und Navigation in LMS. \\ \hline
\textbf{FU5: Möglichkeiten und Grenzen} & Synthese aller Ergebnisse und theoretische Reflexion zur Identifikation von Potenzialen und Limitationen von LMS. \\ \hline
\textbf{FU6: LMS als Kompetenzerwerbssystem} & Systematische Literaturrecherche (500 Quellen) und systemische Bewertung der LMS als Kompetenzerwerbssystem. \\ \hline
\textbf{FU7: Erweiterung von Kausalgesetzen} & Python-Simulation zur Analyse der Korrelationen zwischen Forschungsunterfragen sowie Schlagwort- und Kategorisierungsanalyse. \\ \hline
\end{tabularx}
}
\vspace{0.3cm}
\begin{quote}
{\fontsize{8}{10}\selectfont
Die Tabelle zeigt die Zuordnung der Methoden zu den Forschungsunterfragen. Sie umfasst systematische Literaturanalyse, empirische Erhebungen (Umfragen, Eye-Tracking) und modellbasierte Ansätze (Simulation), die flexibel kombiniert wurden, um die Forschungsfragen adäquat zu beantworten.
}
\end{quote}

\end{table}
```


Tabelle 5: Forschungsunterfragen und deren Bearbeitungsmethoden
Forschungsunterfrage	Methoden
FU1: Akzeptanz und Nützlichkeit	Systematische Literaturanalyse, KI-gestützte Analyse zur Untersuchung der Benutzerfreundlichkeit und Nützlichkeit von LMS.
FU2a: Effekt auf Lernende	Monte-Carlo-Simulation und 3D-Kompetenzmessmodell zur Modellierung der Kompetenzentwicklung und Unsicherheiten im Lernprozess.
FU2b: Effekt auf Lehrende	Umfrage zur digitalen Kompetenz von Lehrenden und ergänzende Literaturanalyse zur Erfassung der Herausforderungen und Bedürfnisse.
FU3: Didaktische und technologische Merkmale	Literaturanalyse, Eye-Tracking-Experiment und systemische Analyse zur Untersuchung der didaktischen und technischen Gestaltung von LMS.
FU4a: Bildungswissenschaftliche Mechanismen	Literaturanalyse zu bildungswissenschaftlichen Modellen und Mechanismen.
FU4b: Technisch-gestalterische Mechanismen	Eye-Tracking-Experiment mit begleitender Umfrage zur Analyse der Wahrnehmung und Navigation in LMS.
FU5: Möglichkeiten und Grenzen	Synthese aller Ergebnisse und theoretische Reflexion zur Identifikation von Potenzialen und Limitationen von LMS.
FU6: LMS als Kompetenzerwerbssystem	Systematische Literaturrecherche (500 Quellen) und systemische Bewertung der LMS als Kompetenzerwerbssystem.
FU7: Erweiterung von Kausalgesetzen	Python-Simulation zur Analyse der Korrelationen zwischen Forschungsunterfragen sowie Schlagwort- und Kategorisierungsanalyse.
Die Tabelle zeigt die Zuordnung der Methoden zu den Forschungsunterfragen. Sie umfasst systematische Literaturanalyse, empirische Erhebungen (Umfragen, Eye-Tracking) und modellbasierte Ansätze (Simulation), die flexibel kombiniert wurden, um die Forschungsfragen adäquat zu beantworten.
Die methodische Abbildung und Zuordnung der Forschungsunterfragen (FU) in Tabelle 4 ist ein zentrales Element des Forschungsdesigns, da sie die Verbindung zwischen den zentralen Erkenntnisinteressen und den jeweils angewandten wissenschaftlichen Methoden systematisch herstellt. Im Folgenden wird eine umfassende, tiefgreifende Ausführung zu den einzelnen Elementen von Tabelle 4 präsentiert. Diese Darstellung erläutert die Hintergründe der gewählten Methoden, ihre Einbettung in die übergeordneten Forschungsfragen sowie die praktischen und theoretischen Implikationen, die sich daraus für das Gesamtvorhaben ergeben. Tabelle 4 ordnet jede einzelne Forschungsunterfrage einer oder mehreren spezifischen methodischen Herangehensweisen zu. Die Zuordnung erfolgt dabei nicht willkürlich, sondern basiert auf einer sorgfältigen Abwägung der jeweiligen Erkenntnisinteressen sowie der Stärken und Grenzen der zur Verfügung stehenden Methoden. Auf diese Weise wird gewährleistet, dass sowohl Breite als auch Tiefe methodischer Zugänge realisiert werden; von rein qualitativen, über quantitative bis hin zu experimentellen Verfahren. Die Tabelle fördert die Transparenz des Forschungsprozesses und macht die Logik der methodischen Wahl für Außenstehende nachvollziehbar.


3	Detailanalyse der Forschungsunterfragen und Methoden
3.1	FU4a: Bildungswissenschaftliche Mechanismen
Forschungsunterfrage:
Welche bildungswissenschaftlichen Modelle und Mechanismen sind für das Verständnis von Lernmanagementsystemen (LMS) zentral?
Methodik:
•	Literaturanalyse zu bildungswissenschaftlichen Modellen und Mechanismen: Die Durchführung einer systematischen Literaturanalyse bildet die methodische Grundlage, um das breite Spektrum bildungswissenschaftlicher Theorien, Paradigmen und Modelle zu identifizieren, die für den Einsatz und das Design von LMS relevant sind. Hierbei erfolgt eine gezielte Recherche in einschlägigen Datenbanken unter Nutzung definierter Schlagworte und Filtermechanismen, um die Qualität und Aktualität der Literatur sicherzustellen.
Begründung & Umsetzung:
Die Literaturanalyse ermöglicht die Zusammenführung und kritische Bewertung von Forschungssträngen, Konzepten und empirischen Befunden. Sie legt den theoretischen Grundstein für alle weiteren empirischen und analytischen Schritte, indem sie zentrale Begrifflichkeiten, Modelle (wie z.B. konstruktivistische Lerntheorien, Motivationstheorie, Kompetenzmodelle etc.) und Mechanismen aufarbeitet. Die Ergebnisse dieser Analyse werden im weiteren Verlauf mit empirischen Daten aus anderen Methoden trianguliert.
3.2	FU4b: Technisch-gestalterische Mechanismen
Forschungsunterfrage:
Wie nehmen Nutzer*innen die technische Gestaltung und Navigation in LMS wahr, und welche Mechanismen beeinflussen den Umgang mit solchen Systemen?
Methodik:
•	Eye-Tracking-Experiment: Diese Methode ermöglicht die objektive Messung von Blickbewegungen, Verweildauer auf bestimmten Elementen, sowie Navigationspfaden von Nutzer*innen innerhalb des LMS. Dadurch werden unbewusste Wahrnehmungs- und Navigationsmuster sichtbar.
•	Begleitende Umfrage: Qualitative und quantitative Erhebungen liefern ergänzende Einblicke in subjektive Präferenzen, Schwierigkeiten und Verbesserungsvorschläge hinsichtlich der Bedienbarkeit und Gestaltung des Systems.
Begründung & Umsetzung:
Die Kombination aus Eye-Tracking-Daten und Umfrageergebnissen ermöglicht eine multidimensionale Betrachtung technisch-gestalterischer Mechanismen. Während Eye-Tracking objektive, verhaltensnahe Daten liefert, erfassen Umfragen subjektive Einschätzungen und Erklärungen. Diese methodische Kopplung eröffnet die Möglichkeit, Diskrepanzen zwischen tatsächlicher Nutzung und Selbstauskunft analytisch zu erfassen.
3.3	FU5: Möglichkeiten und Grenzen
Forschungsunterfrage:
Welche Potenziale und Limitationen bieten LMS aus bildungswissenschaftlicher, technischer und nutzerorientierter Perspektive?
Methodik:
•	Synthese aller Ergebnisse: Auf Basis der aus den vorhergehenden Methoden gewonnenen Daten erfolgt eine umfassende Synthese, in der Möglichkeiten und Grenzen systematisch herausgearbeitet und reflektiert werden.
•	Theoretische Reflexion: Die Ergebnisse werden in einen größeren theoretischen Kontext eingebettet, wodurch auch blinde Flecken und Desiderate offengelegt werden.
Begründung & Umsetzung:
Die Synthese und Reflexion stehen am Endpunkt des Forschungsprozesses und bündeln alle bislang gewonnenen Erkenntnisse. Hierbei werden u.a. die Ergebnisse der Literaturanalyse, empirische Daten aus Experimenten und Umfragen sowie die Resultate aus Simulationen vereint. Ziel ist es, über die isolierte Betrachtung einzelner Aspekte hinauszugehen und ein integratives Bild der Möglichkeiten und Grenzen von LMS zu zeichnen.
3.4	FU6: LMS als Kompetenzerwerbssystem
Forschungsunterfrage:
Inwieweit eignen sich LMS als Systeme für den Kompetenzerwerb, und welche Bedingungen fördern oder behindern einen erfolgreichen Kompetenzerwerb?
Methodik:
•	Systematische Literaturrecherche (ca. 500 Quellen): Umfangreiche Recherche in internationalen Fachzeitschriften, Datenbanken und Konferenzberichten, um die Rolle von LMS als Systeme für den Kompetenzerwerb aus unterschiedlichsten Perspektiven zu beleuchten.
•	Systemische Bewertung: Kritische Analyse und Bewertung der identifizierten Literatur unter Berücksichtigung von Effektivität, Akzeptanz, Implementierungsbedingungen und nachhaltigem Kompetenzaufbau.
Begründung & Umsetzung:
Durch die gezielte systematische Recherche und anschließende Bewertung der Quellen entsteht eine solide empirisch-theoretische Basis für die Beurteilung der Eignung und Grenzen von LMS im Kontext des Kompetenzerwerbs. Die große Anzahl der einbezogenen Quellen gewährleistet eine hohe inhaltliche Tiefe und thematische Breite, wodurch auch selten betrachtete Aspekte identifiziert werden können.
3.5	FU7: Erweiterung von Kausalgesetzen
Forschungsunterfrage:
Wie lassen sich Korrelationen und Zusammenhänge zwischen den Forschungsunterfragen und zentralen Konzepten systematisch analysieren und visualisieren?
Methodik:
•	Python-Simulation: Entwicklung und Durchführung von Simulationen, die verschiedene hypothetische Szenarien und deren Auswirkungen auf die Korrelationen zwischen Forschungsunterfragen abbilden. Hier kommen numerische Verfahren und Algorithmen zur Anwendung, die große Datenmengen verarbeiten können.
•	Schlagwort- und Kategorisierungsanalyse: Mithilfe fortgeschrittener KI-gestützter Verfahren werden thematische Schlagworte und Kategorien aus den erhobenen Daten extrahiert und ausgewertet, um Zusammenhänge und Muster sichtbar zu machen.
Begründung & Umsetzung:
Simulationen bieten die Möglichkeit, komplexe Kausalzusammenhänge und Rückkopplungseffekte experimentell zu erfassen, ohne auf reale Proband*innendaten angewiesen zu sein. Die Kategorisierungsanalyse unterstützt die Identifikation von Mustern und clustert große Informationsmengen, was wiederum Rückschlüsse auf übergeordnete Gesetzmäßigkeiten und Korrelationen zwischen verschiedenen Forschungsaspekten zulässt.
4	Zusammenspiel der Methoden
Die Methoden sind nicht isoliert zu betrachten, sondern stehen in einem engen Wechselverhältnis. Die systematische Literaturanalyse (FU4a, FU6) liefert den theoretischen Rahmen, während Eye-Tracking-Experimente und Umfragen (FU4b) konkrete empirische Daten zur Anwendung und Nutzer*innenperspektive bereitstellen. Simulationen und KI-gestützte Analysen (FU7) ermöglichen es, aus den gewonnenen Daten neue Hypothesen zu generieren und Zusammenhänge auf einer abstrakten Ebene zu modellieren. Die abschließende Synthese (FU5) integriert alle Ergebnisse, reflektiert methodische Limitationen und weist auf offene Forschungsfragen hin.
5	Reflexion der methodischen Vielfalt
Die breite methodische Aufstellung in Tabelle 4 verdeutlicht den Anspruch, das Untersuchungsfeld aus multiplen Perspektiven zu erfassen. Die Kombination aus klassischen (z.B. Literaturanalysen), modernen (z.B. KI-gestützte Analyse) und experimentellen Methoden (z.B. Eye-Tracking, Simulationen) sorgt für eine hohe Validität und Reliabilität der Ergebnisse. Zudem erlaubt sie die flexible Anpassung des Forschungsdesigns an neue Erkenntnisse und sich verändernde Herausforderungen im Forschungsprozess.
Tabelle 4 kann damit als zentrales Steuerungsinstrument des Forschungsprojekts angesehen werden und dokumentiert die bewusste, theoriebasierte und empirisch begründete Zuordnung von Methoden zu Forschungsunterfragen. Sie schafft Transparenz, Nachvollziehbarkeit und sorgt für eine effiziente Planung und Umsetzung der Untersuchung. Die methodische Vielfalt und Tiefe ermöglicht es, sowohl grundlegende Mechanismen als auch spezifische Anwendungsaspekte von LMS umfassend zu analysieren und damit einen substanziellen Beitrag zur wissenschaftlichen Diskussion über das Potenzial digitaler Lernumgebungen zu leisten.
Datenerhebung und -aufbereitung (ca. 4–5 Seiten)
Interpretation der Methodenkohärenz

Durch die kombinierte Anwendung deduktiver Strukturierung, statistischer Clusteranalyse ($k$-Means) und selektiver Segmentierung anhand empirisch ermittelter Perzentile ($Q_2$–$Q_3$) wurde ein qualitativer Literaturkorpus in eine quantitativ auswertbare Struktur überführt. Diese Vorgehensweise ermöglicht eine fundierte Zuordnung der Forschungsunterfragen (FU) zu jenen Bereichen der Literatur, die sich im Verdichtungsraum epistemischer Kohärenz befinden. Das bedeutet: Die ausgewählten Publikationen weisen eine besonders hohe strukturelle Nähe zueinander auf, gemessen an der maximalen Trennschärfe und Homogenität im Silhouette-Score.

Bemerkenswert ist dabei, dass die Auswahl nicht durch subjektives Eingreifen, thematische Vorannahmen oder bewusste Schwerpunktsetzungen erfolgte, sondern ausschließlich durch algorithmisch rekonstruierte Dichtefelder innerhalb der deduktiv-numerischen Vektorräume. Die Aussagen, die aus diesem Literaturfeld hervorgehen, können somit als besonders stabil, kohärent und epistemisch tragfähig gewertet werden – sie stellen gewissermaßen den empirischen Kern des aktuellen Diskurses dar.

Jahr	$n$	Cluster	Silhouette-Skore
2010	7	2	1.0000
2011	29	4	0.9655
2012	7	3	0.8571
2013	28	4	1.0000
2014	24	4	0.9583
2015	28	3	1.0000
2016	25	3	1.0000
2017	98	3	1.0000
2018	95	4	0.9895
2019	202	3	1.0000
2020	303	4	0.9968
2021	377	4	0.9854
2022	430	4	0.9916
2023	899	4	0.9702
2024	780	4	0.9208
2025	192	4	0.9696
Summe			









Die Visualisierung zeigt eine Kombination aus zwei zentralen Metriken für deine Analysequalität:
    Silhouette-Scores (blaue Linie, linke Y-Achse): Ein Maß für die Kohärenz und Trennschärfe der Clusterbildung.
    Fallzahlen n (graue Balken, rechte Y-Achse): Anzahl der analysierten Einträge pro Jahr.

Interpretation der Kurve

1. 
Allgemeines Muster (2010–2025):
    Von 2010–2017 zeigen sich trotz geringer Fallzahlen ($n < 100$) durchweg exzellente Silhouette-Scores (≈1.0).
    2018–2022 bleibt der SC durchgehend über dem Median ($Q_2 \approx 0.9906$), bei gleichzeitig signifikant steigenden Fallzahlen.
    Ab 2023 fällt der Score unter $Q_3$ und erreicht 2024 einen Tiefpunkt von 0.9208, während die Fallzahl mit 780 hoch bleibt.
    2025 zeigt sich eine leichte Erholung des SC (0.9696), jedoch bei stark gesunkenen Fallzahlen.

2. 
Quartile & Bias-Schwellen:
    Q1 (≈ 0.9686): Markiert die Schwelle, ab der Werte als „niedrig“ gelten.
    $Q_3 = 1.0000$: Zeigt an, dass ein Viertel aller Jahre perfekte Clusterkohärenz aufweist – ein selten hoher Wert.
    Fatigue Threshold ($0.96$): Ab diesem Wert könnte eine inhaltliche Erschöpfung im Datenraum interpretiert werden.
    Circadian Optimum & Winsorisierter Median (≈ 0.9906): Dienen als kognitives Optimum bzw. robuste Mittelwerte der Analysequalität.

Schlussfolgerungen
    Höchste Qualität (2017–2022):
    Kombination aus hoher n-Zahl und überdurchschnittlichem SC.
    Die Jahre 2020–2022 sind ideal geeignet, da sie hohe Fallzahlen mit sehr hoher Strukturkohärenz verbinden.
    Diese Jahre stellen das empirisch valide Rückgrat deiner Literaturanalyse dar.
    Frühe Jahre (2010–2016):
    Extrem hoher SC bei kleinen Fallzahlen.
    Inhaltlich hochwertig, aber eingeschränkte Generalisierbarkeit.
    Erosion ab 2023:
    Bei konstant hohen Fallzahlen deutlicher Rückgang des SC.
    Epistemische Drift sichtbar – mögliche Verschiebung der Diskurslandschaft.
    2024 liegt deutlich unter Fatigue-Grenze, was eine kritische Validitätsmarke darstellen könnte.


Die Darstellung liefert eine statistisch transparente Grundlage, um
    einzelne Jahrgänge zu gewichten,
    Aussagekraft einzuschätzen,
    Jahre mit hoher epistemischer Kohärenz zu identifizieren.

Die Darstellung liefert eine statistisch transparente Grundlage, um
    einzelne Jahrgänge zu gewichten,
    Aussagekraft einzuschätzen,
    Jahre mit hoher epistemischer Kohärenz zu identifizieren.

Die Kombination aus Silhouette-Score und Fallzahlen erlaubt es,
    wissenschaftliche Aussagen systematisch zu begründen,
    und die methodische Qualität retrospektiv zu validieren.


KI-gestützte Dokumentenanalyse
- Systematische Dokumentenanalyse:


Die systematische Literaturanalyse orientiert sich methodisch an den Prinzipien der Dokumentenanalyse, wie sie beispielsweise von Döring (2023a, Kapitel 10.6) dargelegt  werden. Ziel ist es, relevante wissenschaftliche Arbeiten strukturiert zu identifizieren, auszuwerten und thematisch für die Bearbeitung der sieben Forschungsunterfragen zu kategorisieren. Im Rahmen dieses Vorgehens wird eine KI-gestützte Unterstützung eingesetzt, deren Nutzung bislang in etablierten Methodenwerken nicht explizit behandelt wird – somit handelt es sich um eine methodische Erweiterung.
Entsprechend der Überlegungen von Yu et al. (2024, S. 2–3, 6–8), die den Einsatz generativer KI zur Förderung von Reflexions- und Analyseprozessen im Forschungsprozess systematisch analysieren, wird diese methodische Innovation theoretisch fundiert begründet. Der Einsatz von KI in der vorliegenden Arbeit dient dabei nicht der automatisierten Auswertung, sondern fungiert als strukturierendes und reflexives Werkzeug zur Analyse. Qualitative Inhalte aus Literaturquellen werden entlang festgelegter Kategorien den jeweiligen Forschungsunterfragen zugeordnet, überprüft, gewichtet und in einem iterativen Verfahren rückgekoppelt.
Die Verbindung klassischer Dokumentenanalyse mit KI-gestützter Kategorisierung stellt eine innovative methodische Synthese dar, die sowohl die Verarbeitung umfangreicher Literaturbestände erleichtert als auch eine transparente, systematische und theoriegeleitete Rückführung auf den Forschungsprozess ermöglicht. Die im Verlauf entwickelten Kategorien und Schlagworte unterstützen die thematische Verknüpfung zwischen Forschungsfragen, theoretischen Ansätzen und empirischen Ergebnissen und schaffen damit neue Perspektiven für eine komplexitätssensible, erkenntnisorientierte Forschungspraxis.




Keine KI-Analyse Erwähnung bei den herkömmlichen Methodenbüchern -> also eine neue Methode – Grundlage (Yu et al., 2024, S. 2–3, 6–8) da dieser explizit die Nutzung von KI zur Unterstützung von Reflexions- und Analyseprozessen behandelt. Ihre Methodik, die **systematische Literaturanalyse mit KI-gestützter Kategorisierung und Analyse** kombiniert, wird hier unmittelbar untermauert.

Die systematische Literaturanalyse in dieser Arbeit dient der Identifikation relevanter Arbeiten zur Beantwortung der Forschungsfragen mit dem Ziel, eine thematische Verknüpfung zwischen den einzelnen Forschungsfragen und bestehenden wissenschaftlichen Erkenntnissen herzustellen. Um die Analyseprozesse effizienter zu gestalten und neue Muster und Zusammenhänge zu identifizieren, wird die systematische Literaturauswertung durch KI-gestützte Methoden ergänzt. Diese hybride Vorgehensweise erlaubt damit, Schlagworte, Kategorien und Querverweise datenbasiert zu strukturieren und so eine methodisch fundierte Grundlage für die weiteren Forschungsschritte zu schaffen.
Ein vergleichbarer Ansatz wird von Yu et al. (2024, S. 2–3, 6–8) beschrieben, die in ihrer Arbeit aufzeigen, dass KI-gestützte Analysen die Tiefe der Reflexion und Analyseprozesse signifikant verbessern können. In ihrem Hybrid Intelligence Feedback (HIF)-System werden große Sprachmodelle (LLMs) genutzt, um Peer-Feedback strukturiert aufzubereiten und in übergeordnete Kategorien zu systematisieren. Diese Vorgehensweise zeigt, dass KI nicht nur eine unterstützende, sondern eine strategisch integrierte Rolle in der systematischen Analyse übernehmen kann. Analog dazu wird in dieser Arbeit eine KI-gestützte Kategorisierung der Literatur durchgeführt, um die Identifikation relevanter Begriffe und Forschungszusammenhänge zu optimieren.
Die Entwicklung von Kategorien und Schlagworten erfolgt in einem iterativen Prozess, bei dem KI-gestützte Verfahren zur Validierung und Optimierung der Kategorisierung genutzt werden. Während klassische systematische Literaturanalyse-Ansätze auf manueller Kodierung basieren, erlaubt der Einsatz von KI eine dynamische Anpassung der Suchbegriffe, Cluster-Bildung und Korrelationen zwischen verschiedenen Forschungsfragen und Themenfeldern. Yu et al. (2024) betonen, dass derartige hybride Systeme dazu beitragen können, unentdeckte Zusammenhänge sichtbar zu machen und neue Forschungsperspektiven zu eröffnen. In dieser Arbeit wird dieser Gedanke erweitert, indem nicht nur die Strukturierung, sondern auch die datengetriebene Absicherung von Kategorien durch KI erfolgt, um eine methodisch belastbare Grundlage für die Forschungsergebnisse zu schaffen.
Die KI-gestützte Analyse zeigte sich als zuverlässig bei der Identifikation thematisch unpassender Forschungsarbeiten, selbst wenn die automatisierte Stichwortsuche in Zotero zunächst eine positive Zuordnung ergab. Ein anschauliches Beispiel hierfür ist die zur Analyse bereit gestellte Studie von Westlake und Mahan (2023), die sich mit BDSM-Praktiken, deren Demografie und Motivationen befasst. Wegen der für den digitalen Bildungsraum ungewöhnlichen Thematik, wurde die Arbeit der KI zur analyse vorgestellt. Da der Forschungsschwerpunkt dieser Arbeit außerhalb des Kontexts digitaler Bildungsräume liegt, weist die KI-Analyse darauf hin, dass sie für die Untersuchung von Learning-Management-Systemen und digitalem Lernen in Gesundheitsberufen nicht relevant ist. Die Analyse führt dies folgendermaßen aus:
"Es scheint, dass die hochgeladene Studie von Westlake und Mahan (2023) nicht direkt in den Rahmen digitaler Bildungsräume passt. Die Arbeit befasst sich mit BDSM-Praktiken, deren Demografie und Motivationen, während dein Fokus auf Learning-Management-Systemen und digitalem Lernen in Gesundheitsberufen liegt. Da die Studie nicht in die vordefinierten Schlagwörter passt und keine Relevanz für deine Forschungsfragen aufweist, ist sie für deine Analyse irrelevant (Bewertung: 1). Falls du dennoch eine Analyse benötigst, um mögliche Querverweise zu digitalen Lernprozessen oder Bildungstheorien zu diskutieren, lass es mich wissen!"
Dieses Beispiel verdeutlicht, wie zuverlässig die KI-gestützte Analyse über eine rein syntaktische Stichwortsuche hinausgeht und in der Lage ist, den inhaltlichen Kontext wissenschaftlicher Arbeiten zu erfassen. Dadurch wird eine gezielte Selektion relevanter Quellen ermöglicht, während zugleich potenziell irreführende Ergebnisse aus der Stichwortsuche systematisch überprüft und ausgeschlossen werden können.

Vergleich der Kodierergebnisse zwischen Mensch und KI

Ein zentraler Aspekt der qualitativen Clustervalidierung ist der Vergleich zwischen menschlichen Kodierungen und KI-gestützten Inhaltsanalysen. Um die methodische Präzision beider Ansätze zu bewerten, wurden die Silhouette-Scores der jeweiligen Analysen berechnet. Die Ergebnisse zeigen deutliche Unterschiede in der Trennschärfe der Cluster.

### Vergleich der Silhouette-Scores: KI-gestützte Analyse vs. menschliche Kodierung
Zur Überprüfung der methodischen Präzision und Trennschärfe von KI-gestützten Analysen im Vergleich zu menschlichen Kodierungen wurde die qualitative Clustervalidierung auf eine klassisch kodierte Studie von Kerman et al. (2024) angewendet. Ziel war es, die Clusterstruktur beider Verfahren zu vergleichen und Unterschiede in der methodischen Konsistenz zu identifizieren.
Die Analyse ergab, dass die KI-gestützte Analyse einen Silhouette-Score von 0.92 erreichte, während die menschliche Kodierung lediglich 0.62 betrug. Dies verdeutlicht die höhere methodische Präzision und Trennschärfe der KI-gestützten Analyse. Während die manuelle Kodierung stärkere Überschneidungen zwischen Kategorien aufwies, erzeugte die KI-gestützte Analyse klar abgegrenzte Clusterstrukturen mit geringerem inhaltlichem Überlapp.
Die Ergebnisse zeigen, dass KI-gestützte Inhaltsanalysen eine objektivere und methodisch konsistentere Alternative zur klassischen Kodierung darstellen können. Die qualitative Clustervalidierung bestätigt, dass menschliche Kodierungsprozesse anfällig für subjektive Einflüsse sind und eine systematische Überprüfung erfordern. Die methodische Stabilität der KI-Analyse verdeutlicht die Notwendigkeit, qualitative Inhaltsanalysen durch datenbasierte Validierung zu ergänzen.
Um die methodische Präzision und Trennschärfe von KI-gestützten Analysen im Vergleich zu menschlichen Kodierungen zu überprüfen, wurde die qualitative Clustervalidierung auf eine klassisch kodierte Studie von Kerman et al. (2024) angewendet. Absicht war der Vergleich der Clusterstruktur zwischen menschlicher Kodierung mit der KI-gestützten Analyse und mögliche Unterschiede in der methodischen Konsistenz zu identifizieren.
Die Analyse ergab, dass die KI-gestützte Analyse einen Silhouette-Score von 0.92 erreichte, während die menschliche Kodierung einen Wert von 0.62 aufwies. Dies zeigt, dass die KI-gestützte Analyse eine deutlich höhere Trennschärfe aufweist und methodisch konsistenter arbeitet. Während die menschlichen Kodierungen inhaltliche Überschneidungen aufwiesen und Kategorien nicht immer klar voneinander abgrenzbar waren, erzeugte die KI-gestützte Analyse präzisere Clusterstrukturen mit geringeren inhaltlichen Überlappungen (vgl. . 
Dieses Ergebnis bestätigt, dass KI-gestützte Inhaltsanalysen methodisch präziser sein können als menschliche Kodierungen. Die qualitative Clustervalidierung zeigt auf, dass menschliche Kodierungsprozesse eine größere Subjektivität aufweisen und daher eine systematische Überprüfung erforderlich ist. Die methodische Stabilität der KI-Analyse verdeutlicht, dass eine datengestützte Validierung menschlicher Kodierungen notwendig ist, um eine methodisch fundierte qualitative Inhaltsanalyse zu gewährleisten.
Die KI-gestützte Analyse erreichte einen Silhouette-Score von 0.92, während die menschliche Kodierung nur einen Wert von 0.62 aufwies. Dies bestätigt, dass KI-gestützte Inhaltsanalysen eine höhere methodische Präzision und Trennschärfe aufweisen als klassische manuelle Kodierungen. Die qualitative Clustervalidierung wurde auf eine klassisch kodierte Studie von Kerman et al. angewendet, um deren methodische Trennschärfe systematisch zu überprüfen und mit einer KI-gestützten Analyse zu vergleichen. Dabei zeigte sich, dass die KI-Analyse klarere Clusterstrukturen erzeugte, während die menschliche Kodierung stärkere Überschneidungen zwischen den Kategorien aufwies. Ein hoher Silhouette-Score deutet auf eine starke Gruppierung der Datenpunkte hin, während ein niedrigerer Wert auf Überlappungen zwischen den Kategorien hindeutet.

### Testansätze

Ein wesentlicher Bestandteil der qualitativen Clustervalidierung ist die systematische Überprüfung der Analyseergebnisse anhand definierter Testansätze. Zunächst erfolgt eine automatische Kodierung, bei der untersucht wird, ob die Methode relevante Konzepte aus dem Text extrahiert und korrekt zuordnet. Anschließend wird die extrahierte Struktur mit der ursprünglichen Kodierung in der Studie verglichen, um mögliche Abweichungen oder Übereinstimmungen zu identifizieren.

Ein weiterer Schritt ist die Clusterbildung mit $k$-Means, um zu prüfen, ob sich inhaltlich sinnvolle Cluster innerhalb der Daten ergeben. Diese werden mit den thematischen Schwerpunkten der Studie abgeglichen, um zu evaluieren, inwiefern die identifizierten Cluster mit etablierten Forschungsstrukturen übereinstimmen.

Zur Stabilitätsprüfung der Analyse wird der Silhouette-Score berechnet, wobei die Clustervalidierungmehrfach durchgeführt wird. Dadurch kann überprüft werden, ob sich die ermittelten Cluster über verschiedene Durchläufe hinweg stabil zeigen oder ob signifikante Schwankungen auftreten. Dies dient als Maß für die methodische Konsistenz der Validierung.

Ein abschließender Vergleich erfolgte durch die Anwendung der qualitativen Clustervalidierung auf die klassisch kodierte Studie von Kerman et al. Dabei wurde analysiert, inwiefern die von Menschen kodierten Kategorien eine ähnlich klare Trennung aufweisen wie die KI-generierten Cluster. Die Ergebnisse zeigen, dass die Clustervalidierung eine objektive Bewertung der bestehenden Kodierung ermöglicht und methodische Schwächen in der menschlichen Kategorisierung sichtbar machen kann.

### Ergänzung zu ATLAS.ti und $k$-Means

In der Diskussion zur methodischen Validierung wurde auch die Möglichkeit betrachtet, klassische Inhaltsanalyse-Tools wie ATLAS.ti 9 oder NVivo für die Analyse KI-generierter Kodierungen einzusetzen. Dabei zeigte sich jedoch, dass diese Werkzeuge primär für die Unterstützung menschlicher Kodierungsprozesse konzipiert sind und keine geeignete Methodik zur objektiven Validierung von Clustern bieten. Die qualitative Clustervalidierung verfolgt hingegen einen anderen Ansatz: Sie nutzt Algorithmen wie $k$-Means nicht zur explorativen Clusterbildung, sondern zur quantitativen Prüfung der methodischen Konsistenz bereits vorhandener Kodierungen. Diese Unterscheidung ist zentral, da die qualitative Clustervalidierung nicht als Konkurrenz zu klassischen Inhaltsanalyseverfahren betrachtet werden sollte, sondern als eine ergänzende Methode zur Überprüfung der Trennschärfe und methodischen Stabilität kodierter Daten.



### Kritische Einordnung bestehender Literatur

In der aktuellen wissenschaftlichen Debatte über die Nutzung von KI in akademischen Kontexten sind zahlreiche Publikationen zu finden, die vor den potenziellen Risiken von KI-generierten Inhalten warnen. Arbeiten wie die von Biswas (2023), Van Niekerk et al. (2025), Storey (2023) und Parker et al. (2024) thematisieren wohl ethische Implikationen, wissenschaftliche Integrität und Herausforderungen im Peer-Review-Prozess. Dabei bleibt jedoch ein entscheidender Aspekt unbeachtet: Bislang existiert keine fundierte empirische Methode zur systematischen Überprüfung der Qualität von KI-generierten wissenschaftlichen Inhalten. Die genannten Studien diskutieren Risiken und Problematiken, liefern dabei keine methodische Grundlage für eine objektive Bewertung der wissenschaftlichen Qualität von KI-generierten Texten. Ein zentrales Defizit dieser Arbeiten besteht in der fehlenden empirischen Prüfung von KI-gestützten wissenschaftlichen Texten. Während argumentiert wird, dass KI-generierte Inhalte problematisch seien, fehlen systematische Vergleiche zwischen KI- und menschlich erstellten Texten sowie methodische Verfahren zur Überprüfung der Trennschärfe von KI-gestützten Analysen. Diese Arbeiten verbleiben weitgehend auf der deskriptiven Ebene und bieten keine quantitativen oder qualitativen Metriken zur Messung der methodischen Präzision von KI-generierten Inhalten.

### Bedeutung für die qualitative Forschung

Die Ergebnisse zeigen, dass die qualitative Clustervalidierung eine objektive Bewertung von Kodierungen ermöglicht und methodische Schwächen sichtbar machen kann. Dies legt nahe, dass KI-gestützte Inhaltsanalysen eine präzisere Ergänzung zur klassischen qualitativen Kodierung darstellen können. Insbesondere in groß angelegten Studien mit umfangreichen Textkorpora könnten KI-basierte Verfahren eine erhebliche methodische Verbesserung ermöglichen.

Gleichzeitig bleibt zu beachten, dass menschliche Kodierungen theoretische Konzepte und interpretative Nuancen einbeziehen können, die über rein datenbasierte Analysen hinausgehen. Diese Erkenntnisse unterstreichen das Potenzial der qualitativen Clustervalidierung als standardisiertes Verfahren zur Überprüfung methodischer Trennschärfe. Langfristig könnte sie als ergänzende Methode zur Qualitätssicherung klassischer Kodierungsverfahren etabliert werden. In der qualitativen Forschung könnte daher ein hybrider Ansatz sinnvoll sein, bei dem KI-gestützte Analysen zur Strukturierung und Validierung menschlicher Kodierungen eingesetzt werden.

---

### Warum ist diese Einbindung hier sinnvoll?
1. **Unmittelbare Parallelen zur Nutzung von KI-gestützten Verfahren in der Literaturanalyse**  
   - Die Arbeit von Yu et al. (2024) belegt, dass KI nicht nur unterstützend wirkt, sondern Reflexions- und Analyseprozesse **strategisch verbessert**.  
   - Dies stärkt Ihre Argumentation, dass Ihre methodische Kombination aus systematischer Literaturanalyse und KI-basierter Kategorisierung ein wissenschaftlich fundierter Ansatz ist.

2. **Verknüpfung mit Ihrer spezifischen Methodik**  
   - Während Yu et al. (2024) KI für Peer-Feedback und Reflexion einsetzen, erweitern Sie diesen Gedanken, indem Sie **KI zur Kategorisierung wissenschaftlicher Literatur und zur Verknüpfung mit Forschungsfragen nutzen**.  
   - Diese Einbindung hebt hervor, dass Ihr Ansatz auf bestehenden Konzepten aufbaut, diese jedoch für ein anderes Anwendungsfeld **weiterentwickelt**.

3. **Positionierung Ihres Beitrags**  
   - Ihre Arbeit wird als **methodische Innovation** sichtbar, indem sie KI-basierte Verfahren nicht nur zur Effizienzsteigerung, sondern auch zur methodischen Validierung der Literaturanalyse nutzt.  

Falls Sie eine **präzisere Integration** oder eine weitere Anpassung an Ihre Argumentationsstruktur wünschen, lassen Sie es mich wissen!





Empirische Methoden
- **Umfrage zu digitalen Kompetenzen**:
  - Ziel: Analyse der Kompetenzen, Herausforderungen und Bedarfe von Lehrenden.
  - Methode: Kombination aus quantitativen und qualitativen Elementen.
- **Eye-Tracking-Experiment**:
  - Ziel: Analyse der visuellen Wahrnehmung und Navigation in LMS.
  - Ergänzung durch begleitende Umfrage:
    - Ziel: Verknüpfung der Eye-Tracking-Daten mit subjektiven Eindrücken.

Simulation und Datenintegration
- **Python-Simulation**:
  - Ziel: Analyse der Korrelationen zwischen Forschungsunterfragen.
  - Ergebnis: Visualisierung der Interdependenzen und strukturellen Kopplungen.
- **Datenintegration**:
  - Verknüpfung der Ergebnisse aus Literatur, Umfragen, Eye-Tracking und Simulation.



Datenanalyse (ca. 4 Seiten)

Verknüpfung qualitativer und quantitativer Daten

- Integration qualitativer (Literatur, Umfragen) und quantitativer (Simulation, Eye-Tracking) Daten.
- Ziel: Systemische Analyse zur Identifikation von Rückkopplungseffekten und emergenten Strukturen.

Kategorien- und Schlagwortanalyse

Das Ziel besteht in der Bildung und Analyse von Kategorien und Schlagworten. Die Methode beinhaltet die Darstellung thematischer Zusammenhänge zwischen den Forschungsunterfragen.

Korrelation und Interdependenz

- Ziel: Analyse der Korrelationen zwischen Forschungsunterfragen mit der Python-Simulation.
- Ergebnis: Visualisierung der strukturellen Kopplungen und Interdependenzen.

Exkurs: Mehrdimensional-analytische Clustervalidierung
Im Zuge der systematischen Literaturarbeit wurde die statistische Clusteranalyse, eher zufällig als potenzielle Erweiterung der qualitativen Analyse in Betracht gezogen (Kapitel 4.3.1). Die Anwendung des $k$-Means-Algorithmus auf einen bereits deduktiv strukturierten Quellenkorpus erschien als vielversprechender Zugang zur Identifikation verborgener Muster oder nicht explizit abgebildeter Strukturen. Überraschenderweise blieben jedoch neue Erkenntnisse aus, da die Clustervalidierung weitgehend die bestehenden semantischen Erkenntnisse bestätigte. Diese zunächst irritierende Stabilität erwies sich im weiteren Verlauf als methodisch hochbedeutsam. Die Tatsache, dass ein klassisch induktiv genutzter Algorithmus ein deduktiv geschaffenes Ordnungssystem reproduzierte, verweist auf eine inhärente Validierung der Ausgangsstruktur. Erst mit zeitlichem Abstand wurde deutlich, dass sich hier eine neue methodische Perspektive eröffnet, d.h. die Möglichkeit, qualitative Strukturierungslogiken algorithmisch zu überprüfen.
Aus dieser Beobachtung entwickelte sich schrittweise die mehrdimensional-analytische Clustervalidierung (mdaCV). Ein Verfahren, das qualitative Strukturierung, algorithmische Clusterdetektion und visuelle Repräsentation in einem konsistenten Validierungsprozess verbindet. Dabei wird ein deduktiv formulierter semantischer Raum entlang inhaltlich begründeter Dimensionen (z. B. Kategorien, Forschungsfragen, Schlagworte) aufgespannt. Die Positionierung der Datenpunkte erfolgt entlang dieser Achsen, die Clusterbildung erfolgt mit dem $k$-Means-Algorithmus, die Qualität der Trennung wird über den Silhouette-Score erfasst (vgl. Rousseeuw, 1987). (Hanisch-Johannsen, 2025a)
Erst in einem späteren Entwicklungsschritt wurde deutlich, dass diese Vorgehensweise nicht nur für menschlich kodierte, sondern auch für KI-generierte Analysen geeignet ist. Durch die Anwendung auf Testdatensätze – real, manipuliert und zufällig – konnte nachgewiesen werden, dass die mdaCV zwischen kohärenten, rauschhaften und künstlich homogenisierten Datenstrukturen zuverlässig differenziert. Die methodische Implementierung wurde versioniert dokumentiert und ist unter folgender Struktur öffentlich einsehbar (https://git.jochen-hanisch.de/promotion/literaturanalyse). Dort finden sich sowohl der vollständige Datensatz mit Testvarianten (Real-, Zufalls- und manipulierte Daten) als auch die korrespondierenden Python-Skripte (analyse_korrelation.py, analyse_netzwerk.py) sowie ein angepasstes .gitignore, zur Sicherstellung, dass keine personenbezogenen oder bibliographisch geschützten Inhalte öffentlich sichtbar sind.
Die Methode wurde nicht abstrakt konzipiert, sondern emergierte aus forschungspraktischen Überlegungen, iterativen Rückkopplungen und der Notwendigkeit, große Datenmengen zugleich strukturiert, nachvollziehbar und validierbar zu analysieren. Die theoretische Herleitung basiert u. a. auf Arbeiten zur Stabilität des $k$-Means-Algorithmus (Rakhlin & Caponnetto, o. J., Kapitel 5), zur Struktur von Merkmalsräumen (Mavroeidis & Marchiori, 2011, Kapitel 3) sowie zur algorithmischen Modellierung semantischer Nähe durch Vektorraummodelle (Mikolov et al., 2013, Kapitel 2). Die mdaCV verbindet somit deduktive Theoriegeleitetheit mit datenbasierter Validierungslogik. Ein methodisches Hybridmodell, das qualitative und quantitative Paradigmen nicht nur überbrückt, sondern integrativ zusammenführt.
Die mdaCV ist ein Verfahren zur Validierung von Kodierungsstrukturen in qualitativ vorstrukturierten Datenräumen. Dieses Verfahren basiert auf einem dreidimensionalen semantischen Raum, in dem Datenpunkte entlang deduktiv definierter Achsen (z. B. Kategorien, Forschungsfragen, Schlagworte) positioniert und anschließend mittels algorithmischer Clustervalidierung überprüft werden. Dabei kombiniert das Verfahren inhaltlich fundierte Dimensionen mit statistischen Bewertungsverfahren wie dem Silhouette-Score (Rousseeuw, 1987, S. 59, 61), um die Trennschärfe und Kohärenz der Clusterbildung zu bewerten.

Die methodische Herleitung fußt auf drei zentralen Komponenten:

1. Deduktive Strukturierung des semantischen Raums : Aufbauend auf theoretisch oder empirisch begründeten Dimensionen erfolgt eine systematische Vorstrukturierung des Datenraums (Kuckartz & Rädiker, 2022; Mayring, 2022; Mayring & Fenzl, 2022). Diese Dimensionen bspw. Kategorien, Disziplinen oder thematische Schlagworte, definieren die Achsen des Raums und ermöglichen die strukturierte Positionierung der Daten.
2. Um die semantische Struktur der Daten algorithmisch analysierbar zu machen, werden begriffliche Relationen in numerische Vektoren überführt. Die semantische Nähe zwischen Datenpunkten entspricht dabei ihrer geometrischen Nähe im Vektorraum. Diese Transformation bildet die Grundlage für distanzbasierte Verfahren wie die Clustervalidierung. Konzepte  wie CBOW und Skip-gram (Mikolov et al., 2013, Kapitel 6) zeigen, dass auch mit vergleichsweise einfachen Modellarchitekturen hochdimensionale, semantisch präzise Repräsentationen berechnet werden können. Dies ermöglicht die effiziente Verarbeitung großer Korpora und bildet die konzeptionelle Basis für die Vektorraummodellierung in der mdaCV.
3. Statistische Validierung mittels $k$-Means-Algorithmus: Die deduktiv vorstrukturierten Daten werden dem $k$-Means-Verfahren unterzogen. Die zentrale mathematische Formulierung basiert auf der Minimierung der quadrierten Distanzen innerhalb der Cluster (Pérez-Ortega et al., 2020, S. 5). Die Wahl der Anzahl der Cluster $k$ erfolgt theoriegeleitet oder wird durch Metriken wie den Silhouette-Score empirisch justiert. Die Sensitivität des $k$-Means-Algorithmus gegenüber strukturellen Varianzen wird dabei bewusst genutzt, um die methodische Konsistenz der Vorstrukturierung zu evaluieren. (Rakhlin & Caponnetto, o. J., Kapitel 6)

Diese Kombination aus inhaltlicher Fundierung, geometrischer Modellierung und algorithmischer Validierung begründet die mdaCV als eigenständiges methodisches Verfahren. Sie wurde im Verlauf der Dissertation iterativ verfeinert, insbesondere durch Tests mit realen, manipulierten und zufälligen Datensätzen, um ihre Robustheit gegenüber Rauschelementen und ihre Fähigkeit  zur Differenzierung inhaltlicher Kohärenz nachzuweisen (Pérez-Ortega et al., 2020, S. 5, Punkt 4). Damit stellt die mdaCV keine bloße Kombination bestehender Verfahren dar, sondern ein transmethodisches Integrationsmodell, das qualitative Kategoriensysteme auf algorithmisch validierbare Weise überprüfbar macht – ein Beitrag zur Qualitätssicherung, Reproduzierbarkeit und epistemischen Transparenz in der qualitativen Bildungsforschung.
Die mehrdimensional-analytische Clustervalidierung begleitete nicht nur den Analyseprozess im engeren Sinne, sondern wurde über den gesamten Promotionszeitraum hinweg als sensible, seismografisch wirkende Dauermessung eingesetzt. Die jeweiligen Messpunkte wurden nach gezielten Veränderungen am Suchbegriffkorpus vorgenommen und erlauben eine fortlaufende Rückmeldung über die semantische Konsistenz des Quellenraums, wobei die Anzahl der Cluster dauerhaft mit n = 4 beibehalten wurde. Im Rahmen dieser Analyse (Achsen: Suchbegriff, Kategorie, Forschungsfrage) wurde der Korpus beispielsweise in einem Prozess zunächst auf n = 3502 Quellen bereinigt, indem bestimmte Dokumentgattungen (etwa Manuskripte oder unspezifische Vorabfassungen) ausgeschlossen wurden. Infolge dieser Kuration stieg der Silhouette-Score von 0.964 auf 0.9751. Diese Differenz ist nicht als bloße numerische Verbesserung zu verstehen, sondern als qualitatives Emergenzphänomen. Nach Einbezug der o.a. Herleitung, wirkt jede Bereinigung in einem semantisch hochdimensionalen Raum potenziell in alle Richtungen. Der Erkenntniswert liegt somit weniger in der absoluten Score-Steigerung, sondern in der damit verbundenen epistemischen Schärfung, die sich durch den Ausschluss semantischer Rauschelemente ergibt. Hier demonstriert die Analyse exemplarisch, wie sich durch dreidimensional deduktive Validierung eine strukturell kohärente Quellenarchitektur rekonstruieren lässt.
Nach erneuter Einbindung der zuvor ausgeschlossenen Konferenzbände stieg die Anzahl der analysierten Quellen auf n = 3572. Überraschenderweise blieb der Silhouette-Score mit 0.9754 nicht nur stabil, sondern übertraf den vorherigen Wert sogar leicht. Dieses Ergebnis legt nahe, dass die dreidimensionale deduktive Validierung hinreichend robust ist, um auch heterogene Dokumenttypen kohärent zu integrieren. Der ursprünglich befürchtete semantische Rausch-Effekt durch Konferenzbeiträge trat nicht ein; vielmehr scheint die zunehmende Datenfülle eine semantische Verdichtung zu bewirken. Das Cluster-Modell reagiert dabei nicht empfindlich, sondern resilient-emergent auf Datenerweiterung.
Die Beobachtungen von Veränderungen innerhalb der mehrdimensional-analytische Clustervalidierungsind insbesondere im Grenzbereich zwischen Systemstabilität und kategorialer Modifikation aufschlussreich. In einem weiterem Durchgang wurde der Eintragstyp Buchteil gezielt untersucht. Dabei wurde der Datensatz minimal um einen Eintrag reduziert (nun n = 3571), woraufhin sich der Silhouette-Score um -0.001 veränderte. Diese Differenz mag numerisch klein erscheinen, ist jedoch im Kontext eines Scores über 0.97 hochrelevant. In diesem Bereich deutet bereits eine Veränderung in der dritten Nachkommastelle auf strukturelle Anpassungen im Clustermodell hin, etwa durch leicht verschobene Clusterzentren oder veränderte Einpassung eines Einzelbeitrags.
Diese hier exemplarisch angedeutete Sensitivität ist Ausdruck der hohen Auflösung und Differenzierungsfähigkeit des Modells. Im Gegensatz zu vielen anderen Clustering-Ansätzen, die bei kleinen Eingriffen stark „springen“, reagiert dieses System kontinuierlich und rückmeldungsfähig. Der Eintragstyp Buchteil könnte beispielhaft eine inhärent variablere semantische Positionierung besitzen, etwa durch seine Funktion als Vorwort, methodischer Einschub oder Randthema. Auch eine Überrepräsentation bestimmter Werke kann potenziell zu Verzerrungen führen. Die gezielte Analyse solcher Subtypen eröffnet Möglichkeiten für weiterführende Fragestellungen: Wie viele Buchteile stammen aus dem gleichen Werk? Welche Achsendimensionen beeinflussen ihre Clusterzuordnung? Und inwieweit führt das gezielte Entfernen einzelner Elemente zu strukturellen Verschiebungen im Modell?
Eine Veränderung von bspw. $0.001$ bei konstantem Stichprobenumfang und stabiler $k$-Means-Architektur stellt eine reale, systemisch interpretierbare Verschiebung dar. Das System reagiert feinfühlig, d.h. auf Einzelbeiträge und dokumentiert deren Auswirkungen auf die Gesamtstruktur. Daraus ergeben sich potenzielle Analysepfade zur Erforschung mikrostruktureller Dynamiken innerhalb epistemisch strukturierter Clusterräume. Wie Tabelle 5 darstellt, überlagern sich nicht nur qualitative und quantitative Paradigmen, sondern verzahnen sich strukturell.
Tabelle 6: Strukturelle Paradigmen-Überlagerung bei Clusteranalysen
Quantitativ	Qualitativ
Silhouette-Score als Gütemaß	Deduktive Kategorienstruktur
Clusterdichte und Trennschärfe	Theoriegeleitete Semantikachsen
$k$-Means als algorithmischer Kern	Vorstrukturierung durch Forschungsperspektiven
Die Darstellung verdeutlicht, wie sich deduktive, theoriegeleitete Kategorien mit algorithmischen, quantitativ validierbaren Verfahren, etwa dem $k$-Means-Algorithmus und dem Silhouette-Score, strukturell verzahnen. Diese methodische Komplementarität ist zentral für die mehrdimensional-analystische Clustervalidierung (mdaCV) und ermöglicht die gleichzeitige Berücksichtigung epistemischer Tiefenstruktur und formaler Trennschärfe.
Besonders hervorzuheben ist dabei, dass die methodische Verzahnung nicht nur eine Erweiterung quantitativer Validierungsmaßstäbe bedeutet, sondern auch die Öffnung für neue, integrative Bewertungsdimensionen. Während die klassische Clusterbewertung meist auf einzelne numerische Kennzahlen fokussiert, rückt der mdaCV-Ansatz die Notwendigkeit einer umfassenderen Güteprüfung ins Zentrum, bei der neben der formalen Trennschärfe auch die inhaltliche Erfassungstiefe und Vollständigkeit der Daten eine Rolle spielt. Damit wird der Blick für latente Verlustrisiken geschärft, die rein metrische Metriken bislang ausblenden.
Epistemische Verlustfunktion als heuristisches Integritätsmaß
Im Kontext der mehrdimensional-analytischen Clustervalidierung wird üblicherweise der Silhouette-Score als zentrales Maß zur Beurteilung der Clusterdifferenzierung genutzt (i.A.a. Rousseeuw, 1987). Dieser Wert allein erfasst jedoch lediglich die geometrische Separierbarkeit der Cluster im Vektorraum. Was bislang fehlt, ist ein zusammengesetztes Maß, das sowohl die strukturelle Kohärenz (Silhouette) als auch die semantische Vollständigkeit (Datenintegrität) einer Analyse widerspiegelt. Im Rahmen dieser Dissertation wurde daher eine epistemische Verlustfunktion ℇ eingeführt, die beide Dimensionen in einem einzigen heuristischen Indikator vereint. Ziel dieses Verfahrens ist die Modellierung eines skalierbaren Integritätsmaßes, welches sowohl den Grad der Clusterdifferenzierung als auch den Umfang erfasster Quellen berücksichtigt. Die Funktion kann damit als Überwachungsgröße für Datenverarbeitungsläufe herangezogen werden und kritische Abweichungen sichtbar machen, die sich nicht allein über Silhouette- oder Dokumentenzahl abbilden lassen. Die epistemische Verlustfunktion wird von den beiden Größen Clusterdifferenzierungsleistung, gemessen über den Silhouette-Score, und Datenvollständigkeit, gemessen über das Verhältnis zwischen intendierter und tatsächlich verarbeiteter Quellenzahl. Die Epistemische Verlustfunktion ℇ wird wie folgt definiert:
Formel 1: Definition der Verlustfunktion

```{=latex}
\begin{equation}
\label{eq:verlust}
\varepsilon = (1 - S) + \frac{n_{\text{Soll}} - n_{\text{Ist}}}{n_{\text{Soll}}}
\end{equation}
```
Diese additive Formulierung bringt zwei unterschiedliche Validitätsaspekte auf eine gemeinsame Skala:
    Struktureller Verlust, formuliert als (1-S), wobei S den Silhouette-Score repräsentiert. Diese Größe misst die Abweichung vom optimalen Clusteringwert S=1. Je niedriger der Silhouette-Score, desto größer ist der Verlust an struktureller Trennschärfe und Clusterkohärenz.
    Datenverlust, formuliert als (((n_Soll- n_Ist ))/n_Soll ). Dieser Term beschreibt den relativen Anteil an Quellen, die nicht in die Analyse einflossen. Je höher der Wert, desto größer ist die epistemische Lücke im analysierten Datenkorpus.
Beide Komponenten sind dimensionslos, additiv kombinierbar und liegen im Werteberich W=[0,2] ]. Die resultierende Funktion ℇ gibt somit eine Gesamtverlustschätzung für die epistemische Integrität eines Analyseverfahrens.
Angenommen, ein Analyse-Korpus umfasst $n_{\text{Soll}} = 3585$ Einträge, in die Clustervalidierung gingen $n_{\text{Ist}} = 3583$ Quellen ein. Der ermittelte Silhouette-Score beträgt $S = 0{,}9754$. Dann ergibt sich:

```{=latex}
\begin{equation*}
\varepsilon = (1 - 0{,}9754) + \frac{2}{3585}
\approx 0{,}0246 + 0{,}000558
\approx 0{,}0252
\end{equation*}
```
Die epistemische Verlustfunktion liegt in diesem Fall mit ≈ 0,0252o in einem sehr niedrigen Bereich. Sie zeigt, dass trotz kleiner Datenverluste und nicht perfekter Trennschärfe eine nahezu optimale Integrität erreicht wurde. Damit bietet somit ℇ eine differenzierte Perspektive auf die Validität einer Analyse und eignet sich insbesondere:
    zur Qualitätssicherung von Analysepipelines (z. B. automatische Literaturanalysen, KI-generierte Korpora),
    zum Vergleich unterschiedlicher Datenverarbeitungen (z. B. real vs. manipuliert vs. zufällig) sowie
    als metawissenschaftliche Monitoring-Größe in dynamischen Forschungsumgebungen.
Der Nutzen dieses Maßes liegt nicht in seiner absoluten Exaktheit, sondern in der epistemischen Sensibilität. Schon kleinste Abweichungen vom Ideal (Silhouette < 1 oder Datenlücken) werden sichtbar gemacht und können reflektiert werden, woraus eine neue Form der kontinuierlichen Gültigkeitsüberwachung in datenintensiven Forschungsprozessen entsteht. Die hier eingeführte epistemische Verlustfunktion ℇ stellt ein heuristisches und gleichzeitig methodisch begründetes Integritätsmaß dar, das den Anspruch der mdaCV auf Verknüpfung qualitativer und quantitativer Güteprinzipien konsequent weiterführt. Sie ist anschlussfähig für weitere Forschungsdesigns, maschinelle Analysen und metawissenschaftliche Validitätsdiskurse.
Reflexion der Methode (ca. 2 Seiten)
Die kritische Methodenreflexion hat den Zweck, die eigene Arbeitsweise transparent, nachvollziehbar und anhand des wissenschaftlichen Qualitätskriteriums „Methodische Strenge“ (Döring, 2023c, S. 89–90) beurteilbar zu machen. Inwiefern diese Arbeit die Anforderungen an eine methodisch saubere, nachvollziehbare und theoriegeleitete Forschung erfüllt, ist in diesem Kapitel zu klären.
Als Herleitungsgrundlage kann ein systemisch-konstruktivistisches Verständnis von Erkenntnis angesetzt werden, das mit bewährten Evaluationsmodellen (z. B. dem CIPP-Modell nach Stufflebeam in Hanisch (2017, Kapitel 3.1)) sowie analytischen Verfahren wie Korrelations- und deduktiven Clusteranalysen verbunden wird. Diese Kombination ist weder beliebig noch additiv, sondern strukturell aufeinander bezogen und somit theoriekompatibel.  Die Auswahl der Methoden ergibt sich aus der forschungsfragengeleiteten Logik. Sie folgt keiner Paradigmentreue, sondern einem funktionalen Verständnis von Methodeneinsatz und hat zur Folge, dass qualitative und quantitative Verfahren entlang der FU dort eingesetzt werden, wo sie zur Bearbeitung beitragen. Die theoretischen Begriffe (z. B. Kompetenz, Selbstorganisation, Nachhaltigkeit) werden auf konkrete Analyseebenen übertragen, etwa über Prädiktorvariablen (z. B. PV1a–PV3 bei Hanisch (2017, Kapitel 3.4)) oder KI-gestützte Analysen. Sämtliche Analyseprozesse, von der Auswahl der Quellen, über die Generierung und Anwendung der Prompts, bis hin zur Auswertung und Rückführung in die FU, sind dokumentiert, versioniert und theoretisch hergeleitet. Die Struktur folgt einer nachvollziehenden analytischen Logik, die von der FU über die erste KI-gestützte Analyse bis zur Metaebene mit Clusterauswertungen übergeht. Als kuratierende Hilfsmittel unterstützen digitale Werkzeuge, unter deren Verwendung das Literatur und Notizmanagement (Zotero), die Versionierungen (Gitea), sowie die statistischen Berechnungen und Visualisierungen (Python) durchgeführt werden konnten. Diese Kombination von Methoden und Werkzeugen gewährleistet sowohl Reproduzierbarkeit als auch in sich Konsistenz.
Bereits in der Zusammenstellung der Analyseeinheiten werden bewusste Entscheidungen getroffen – z. B. zur Nichtberücksichtigung von Masterarbeiten und reiner „grauer Literatur“ in bestimmten Clusteranalysen. Diese werden nicht nur transparent dargestellt, sondern auch theoriebezogen begründet. Dadurch erhöht sich die Validität der Aussagen.
Ein wesentlicher Bestandteil meines methodischen Vorgehens ist die fortlaufende Selbstprüfung und Justierung. Dazu gehören die Prüfung der Wirksamkeit der Prompts, die Diskussion der Silhouette-Werte zur Clustertrennschärfe, aber auch die bewusste Unterscheidung zwischen Analysen 1. Ordnung (einzelne Quelle) und Analysen 2. Ordnung (übergreifende Auswertung, Rückführung auf die FU).
Mein methodisches Vorgehen erfüllt – trotz seiner systemisch-flexiblen Struktur – zentrale Anforderungen wissenschaftlicher Strenge: Die Methoden sind theoriebasiert, nachvollziehbar, funktional gewählt und systematisch eingesetzt. Gleichzeitig erweitere ich die bestehende Methodendiskussion durch den reflektierten Einsatz generativer KI als epistemisches Werkzeug und durch die Integration klassischer Evaluationsverfahren in ein offenes, komplexitätssensibles Design.

Diese Vorgehensweise ist – so meine Einschätzung – nicht nur methodisch tragfähig, sondern auch ein konkreter Beitrag zur Weiterentwicklung digital-epistemischer Forschung in Bildungssettings.
Selbstverständlich muss im Sinne der wissenschaftlichen Redlichkeit (Döring, 2023c, S. 130–131), und in Anbetracht der aktuellen kritischen Haltung  gegenüber generativen bzw. künstlichen Intelligenzen das hier gewählte methodische Vorgehen nicht nur dargelegt, sondern im besonderen Maße nachvollziehbar erläutert werden. Als Grund für diese Erklärung kann angeführt werden, dass die wissenschaftliche Eigenleistung infrage gestellt werden kann, wenn die Analysen GPT-basiert durchgeführt werden. Das methodische Vorgehen, d.h. die Durchführung inhaltsanalytischer Einzelanalysen mithilfe von GPT und deren anschließende Zusammenführung durch eine deduktive, auf Forschungsunterfragen ausgerichtete Cluster- und Metaanalyse, stellt eine eigenständige wissenschaftliche Leistung dar. Diese kann durch folgende Begründungslogik belegt werden:
    Selbständige Definition erkenntnisleitender Kategorien: Die zugrunde liegenden Kategorien und Kodierungen (wie z.B. „Akzeptanz“, „Nützlichkeit“, „Effekt“, „Gestaltung“ u. a.) wurden aus den Forschungsunterfragen eigenständig abgeleitet. Diese Kategorien sind als deduktive Filter anzusehen, welche die Ausrichtung und Vergleichbarkeit der GPT-gestützten Einzelanalysen ermöglichen. Ohne diese Struktur blieben die Ergebnisse der Analysen unsystematisch und nicht aggregierbar.
    Eigenständige wissenschaftliche Durchführung der Metaanalyse: Die Analysen führen zu keiner Aggregation klassischer Primärforschungsergebnisse, sondern werden zu semantisch strukturierten, vorbereiteten GPT-Einzelanalysen verdichtet. Diese enthalten bereits wissenschaftliche Extrakte, deren Struktur vorgegeben wird. In einem weiteren Schritt wird geprüft, ob die Ergebnisse im Hinblick auf die Forschungsunterfragen widerspruchsfrei, konsistent und saturiert sind. Strukturell entspricht dies einem theoriegeleiteten Validierungsschritt, wobei sowohl die analytischen Kategorien als auch die Aussagekraft der Analysen überprüft werden. 
    GPT als analytisches Werkzeug, nicht als Urheberschaft: GPT wird ausschließlich als analytisches Instrument eingesetzt, vergleichbar mit etablierten Softwarelösungen wie SPSS oder MaxQDA. Die Verantwortung für Struktur, Steuerung und Auswertung lagen zu jeder Zeit vollständig in der eigenen Hand. Die wissenschaftliche Eigenständigkeit resultiert somit nicht aus der Textgenerierung, sondern aus der theoretischen Fundierung und Auswertung der Ergebnisse.
    Geschlossenes System analytischer Selbstreferenz: Das Verfahren umfasst einen zyklischen Prozess: vom Theorierahmen über die empirische Anreicherung, die GPT-Analyse erster Ordnung, die Clusterdarstellung bis hin zur Rückbindung an die handlungsleitenden Forschungsunterfragen. Diese Form rekursiver Validierung stellt ein fortgeschrittenes und bislang wenig beschriebenes methodologisches Vorgehen dar.
    Beitrag zur wissenschaftstheoretischen Innovation: Das Vorgehen erfüllt Kriterien einer strengen Operationalisierung, methodischen Reflexion über Automatisierungsprozesse sowie einer systematischen Steuerung von KI als Analyse- und Verdichtungsinstrument. Damit entsteht ein möglicher methodologischer Prototyp für KI-unterstützte Metaforschung.
Infolgedessen liegt die wissenschaftliche Eigenleistung in der Strukturierung des Analyseprozesses, der Definition und Trennung der Ordnungsebenen (1. Ordnung: Analyse, 2. Ordnung: Bewertung), der methodologischen Fundierung (deduktiv und theoriebasiert) sowie in der reflexiven Kontrolle des Systems. Dieses Vorgehen ist originär, transparent dokumentiert und methodologisch innovativ.

Methodische Stärken

- Forschungsfragengeleiteter Ansatz mit systemischer Perspektive.
- Kombination klassischer Methoden (Literatur, Simulation, Eye-Tracking) mit innovativen Ansätzen (KI, Python).

Methodische Herausforderungen und Limitationen

- Herausforderungen:
  - Retrospektive Integration einiger Methoden.
  - Entwicklung eines eigenen Paradigmas zur Bearbeitung der Forschungsfragen.
- Limitationen:
  - Komplexität der Datenintegration.
  - Abhängigkeit von KI-Tools und Simulationen.
