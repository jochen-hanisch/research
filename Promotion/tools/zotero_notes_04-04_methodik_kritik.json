[
  {
    "attachmentKey": "TTPCNK22",
    "shortTitle": "Storey (2023) – AI & Academic Writing",
    "tags": ["Methodenkritik", "GenAI", "Academic Integrity"],
    "noteHtml": "<p><strong>Methodenkritik (Anschlussstelle: KI-gest\u00fctzte Text-/Literaturarbeit, Promptworkflow, Dokumentationslogik)</strong></p><p>Der Beitrag macht sehr klar, dass GenAI den Dissertationsprozess nicht nur \"beschleunigt\", sondern die Trennlinie zwischen eigenst\u00e4ndiger wissenschaftlicher Leistung und maschinell generierter Formulierung/Strukturierung faktisch verwischt: Dissertation Committees stehen vor dem Problem, nicht mehr verl\u00e4sslich unterscheiden zu k\u00f6nnen, was vom Kandidaten stammt und was von KI erzeugt wurde; verst\u00e4rkend kommt hinzu, dass KI-Ausgaben gezielt einen akademischen Stil imitieren k\u00f6nnen und Detektionswerkzeuge (gerade bei neuen Modellen) nicht stabil greifen. Konsequenz f\u00fcr mein Vorgehen ist: KI wird methodisch nicht als \"Autor\" eingesetzt, sondern als probabilistischer Assistenzraum, dessen Outputs zwingend in eine Pr\u00fcf- und Dokumentationslogik zur\u00fcckgebunden werden m\u00fcssen (Audit-Trail: Prompt, Input, Output, Entscheidung, Revision), um die Zuschreibung von Autorenschaft und die Nachvollziehbarkeit der Argumentf\u00fchrung nicht zu unterlaufen. (Storey, 2023, S. 2)</p><p>Gleichzeitig wird ein zweiter, in meiner Methodik zentraler Kritikpunkt adressiert: Auch wenn Hochschulen die Nutzung prinzipiell zulassen, wird die Bedingung der effektiven, ethischen und transparenten Nutzung explizit an die Sorge gekoppelt, dass eine (zu starke) Abh\u00e4ngigkeit von KI die Schreib-, Kritik- und Evaluationskompetenzen des Forschenden erodieren kann. Daraus folgt f\u00fcr mein Design eine doppelte Absicherung: (1) KI-Outputs werden nicht als Erkenntnis akzeptiert, sondern als Vorschlag, der gegen Prim\u00e4rtexte, Ankerbeispiele und theoriegeleitete Kriterien gegengepr\u00fcft wird; (2) die argumentative Verantwortung bleibt beim Forschenden und wird \u00fcber explizite Entscheidungen (Warum diese Kategorie? Warum diese Verdichtung?) sichtbar gehalten. (Storey, 2023, S. 4)</p><p>Schlie\u00dflich wird die Integrit\u00e4tsdimension pragmatisch gefasst: KI-gest\u00fctzte Plagiats- und Formatierungstools k\u00f6nnen zwar genutzt werden, aber nur unter der Bedingung, dass der Einsatz transparent gemacht und korrekt referenziert wird. Das st\u00fctzt in meiner Methodik die Forderung nach Offenlegung (welche Tools wof\u00fcr), ohne damit die inhaltliche Pr\u00fcfpflicht zu delegieren. (Storey, 2023, S. 13)</p>"
  },
  {
    "attachmentKey": "MBIIBEIT",
    "shortTitle": "Giannakos et al. (2024) – Promise & Challenges of GenAI",
    "tags": ["Methodenkritik", "GenAI", "Governance", "Bias", "Privacy"],
    "noteHtml": "<p><strong>Methodenkritik (Anschlussstelle: GenAI als Analyse-/Synthesehilfe, Governance, Validierungslogik)</strong></p><p>Der Beitrag liefert mir eine argumentative Br\u00fccke, um GenAI-Einsatz als methodisch kontrollbed\u00fcrftige Intervention zu rahmen: Sobald GenAI Funktionen in Analyse- und Designprozesse (hier exemplarisch: Problemidentifikation, Elaborationsschritte) einspeisen, muss die Arbeitsgruppe die Kernlimitierungen (insb. Halluzinationen und Bias) aktiv mitdenken und pr\u00fcfen, ob die erzeugten Outputs tats\u00e4chlich die \"Essenz\" der Diskussion bzw. des Materials abbilden. F\u00fcr mein Vorgehen ist das ein direktes Argument f\u00fcr eine explizite Pr\u00fcf- und Dokumentationslogik (Gegenlesen am Prim\u00e4rtext, Kriterienraster, Protokollierung von Entscheidungen), statt GenAI als bequeme Abk\u00fcrzung in der Argumentation zu behandeln. (Giannakos et al., 2024, S. 13)</p><p>Der Text sch\u00e4rft zudem eine oft untersch\u00e4tzte Risikoebene, die f\u00fcr Forschungsarbeit operativ relevant ist: Die Wiederverwendung bzw. das Hochladen sensibler bzw. kommerziell relevanter Inhalte in GenAI-Plattformen wird als konkretes Risiko beschrieben, weil Kontrollverlust \u00fcber Nutzung, Weiterverwertung und Rechte droht. Das st\u00fctzt in meiner Methodik die Notwendigkeit, Datenfl\u00fcsse (welche Inhalte wohin) zu begrenzen und die Tool-Nutzung in datenschutz- und urheberrechtlich robuste Bahnen zu legen. (Giannakos et al., 2024, S. 12)</p><p>Im Kern fordert der Beitrag, technische und organisatorische Sicherungen nicht als \"Nice-to-have\", sondern als Designanforderung zu behandeln: Systeme sollen aktiv Defizite in Transparenz, Accountability, Privacy, Fairness sowie Bias reduzieren; flankierend wird ein Policy-Frame als notwendig markiert. Damit kann ich methodisch begr\u00fcnden, warum ich Governance-Elemente (Leitplanken, Offenlegung, Audit-Trail, Rollen- und Verantwortungszuweisung) als Teil der Methodik mitf\u00fchre, statt sie in ein reines Ethikkapitel auszulagern. (Giannakos et al., 2024, S. 22)</p><p>Schlie\u00dflich liefert der Beitrag ein starkes Argument gegen vorschnelle Validit\u00e4tsbehauptungen: Es wird explizit gefordert, nicht bei \"human-likeness\"-Checks oder punktuellen Leistungsbenchmarks stehen zu bleiben, sondern langfristige (longitudinal) Evaluationen der Effekte und Kompetenzen zu verlangen. Das st\u00fctzt meinen Anspruch, KI-gest\u00fctzte Schritte nur als heuristische Verdichtung zu f\u00fchren, deren Wert sich erst im Zusammenspiel mit empirischer Triangulation und regelgeleiteter Nachpr\u00fcfung stabilisiert. (Giannakos et al., 2024, S. 15)</p>"
  },
  {
    "attachmentKey": "BLDVYCEB",
    "shortTitle": "Papoutsaki et al. (WebGazer) – Webcam Eye Tracking",
    "tags": ["Methodenkritik", "Webcam Eye-Tracking", "Validity", "Measurement Error"],
    "noteHtml": "<p><strong>Methodenkritik (Anschlussstelle: webcam-basierte Eye-Tracking/Attention-Proxies, Messfehler, Interpretationsgrenzen)</strong></p><p>Der Beitrag ist f\u00fcr mich vor allem ein Argument daf\u00fcr, webcam-basiertes Eye-Tracking nicht als pr\u00e4zises Messinstrument, sondern als Skalierungs-/N\u00e4herungsinstrument zu behandeln: WebGazer wird explizit so gerahmt, dass die Genauigkeit \"sufficient\" f\u00fcr eine Approximation des Blickortes ist \u2013 also f\u00fcr grobe Aufmerksamkeitsr\u00e4ume, nicht f\u00fcr feinaufl\u00f6sende Blickpfadinterpretationen. Das passt zu einer Methodik, die Blickdaten nur als probabilistische Zusatzspur f\u00fchrt (Triangulation), nicht als kausale Erkl\u00e4rungsebene. (Papoutsaki et al., 2016, S. 1)</p><p>Operativ begr\u00fcndet der Beitrag zwei harte Limitationen, die ich in der Methodik explizit sichtbar machen muss: (1) Die Pupil-Detection arbeitet mit Annahmen (Iris dunkler, kreisf\u00f6rmig, Pupille im Zentrum), die \"not always true\" sind und nur oft genug funktionieren, um brauchbare Echtzeitergebnisse zu liefern \u2013 damit ist Messfehler systematisch angelegt. (2) Die Selbstkalibrierung basiert auf der (vereinfachten) Annahme, dass Blick- und Klickpositionen bei Interaktionen (Clicks) perfekt ausgerichtet sind; gleichzeitig wird referenziert, dass gaze-cursor distance im Mittel bei Clicks deutlich von Null abweichen kann, was die Kalibriergrundlage methodisch fragil macht. (Papoutsaki et al., 2016, S. 2\u20134)</p><p>Empirisch wird das zudem \u00fcber Fehlermetriken in Pixeln konkretisiert: In der Remote-Studie werden mittlere Fehler in der Gr\u00f6\u00dfenordnung von hunderten Pixeln berichtet (z.B. Mean Error um 256.9px bzw. 233.4px je nach Modell) und selbst in den \"besten\" Konfigurationen liegen Fehlerwerte (z.B. M = 169px bzw. 187px) im Bereich, der f\u00fcr feingranulare Blickinterpretationen schlicht zu grob ist; im Fazit wird WebGazer daher explizit als geeignet f\u00fcr Anwendungen beschrieben, in denen die ungef\u00e4hre Blickposition ausreicht. Das st\u00fctzt in meiner Methode die Entscheidung, Blickdaten nur in aggregierter/robuster Form auszuwerten (z.B. AOI-Grobzonen, robuste Schwellen, Sensitivit\u00e4tschecks) und die Interpretationsanspr\u00fcche strikt zu begrenzen. (Papoutsaki et al., 2016, S. 5\u20136)</p>"
  }
]

